{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c4a23f",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook is used to compare the annotation results on the mini-sets w & w/o the judge and before & after prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd8c4e",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "\n",
    "RESULTS_PATH = \"../../experiments/prompting/\"\n",
    "COLORS = [\"#acb0e0\", \"#bcd5c3\", \"#cd968e\", \"#d7d8d3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0013ba5",
   "metadata": {},
   "source": [
    "### 1. Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b01eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test_input_wo_judge = []\n",
    "baseline_val_input_wo_judge = []\n",
    "baseline_test_input_w_judge = []\n",
    "baseline_val_input_w_judge = []\n",
    "enhanced_test_input_w_judge = []\n",
    "enhanced_val_input_w_judge = []\n",
    "enhanced_test_input_wo_judge = []\n",
    "enhanced_val_input_wo_judge = []\n",
    "\n",
    "for filename in os.listdir(RESULTS_PATH):\n",
    "    with open(RESULTS_PATH + filename) as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    if \"mini_test_set_baseline\" in filename and \"wo_feedback\" in filename:\n",
    "        baseline_test_input_wo_judge.append(metrics)\n",
    "\n",
    "    elif \"mini_val_set_baseline\" in filename and \"wo_feedback\" in filename:\n",
    "        baseline_val_input_wo_judge.append(metrics)\n",
    "\n",
    "    if \"mini_test_set_baseline\" in filename and \"w_feedback\" in filename:\n",
    "        baseline_test_input_w_judge.append(metrics)\n",
    "\n",
    "    elif \"mini_val_set_baseline\" in filename and \"w_feedback\" in filename:\n",
    "        baseline_val_input_w_judge.append(metrics)\n",
    "\n",
    "    elif \"mini_val_set_enhanced\" in filename and \"w_feedback\" in filename:\n",
    "        enhanced_val_input_w_judge.append(metrics)\n",
    "\n",
    "    elif \"mini_test_set_enhanced\" in filename and \"w_feedback\" in filename:\n",
    "        enhanced_test_input_w_judge.append(metrics)\n",
    "\n",
    "    elif \"mini_val_set_enhanced\" in filename and \"wo_feedback\" in filename:\n",
    "        enhanced_val_input_wo_judge.append(metrics)\n",
    "\n",
    "    elif \"mini_test_set_enhanced\" in filename and \"wo_feedback\" in filename:\n",
    "        enhanced_test_input_wo_judge.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(results, experiment_name, set_name):\n",
    "    metrics = {\n",
    "        \"total_token_count_annotator\": [],\n",
    "        \"total_token_count_judge\": [],\n",
    "        \"micro_f1_objects\": [],\n",
    "        \"micro_f1_spans\": [],\n",
    "        \"cosine similarity\": [],\n",
    "        \"Levenshtein distance\": [],\n",
    "        \"delete percentage\": [],\n",
    "        \"false positive percentage\": [],\n",
    "        \"coverage percentage\": [],\n",
    "        \"map_50\": [],\n",
    "        \"map_50_95\": [],\n",
    "    }\n",
    "\n",
    "    for result in results:\n",
    "        for metric in metrics.keys():\n",
    "            if metric in result.keys():\n",
    "                metrics[metric].append(result[metric])\n",
    "            else:\n",
    "                metrics[metric].append(result[\"span_similarity_metrics\"][metric])\n",
    "\n",
    "    return (\n",
    "        pl.from_dict(metrics)\n",
    "        .with_columns(pl.lit(experiment_name).alias(\"experiment\"))\n",
    "        .with_columns(pl.lit(set_name).alias(\"set\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37812c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test = get_results(baseline_test_input_w_judge, \"baseline prompt & judge\", \"test\")\n",
    "baseline_val = get_results(baseline_val_input_w_judge, \"baseline prompt & judge\", \"val\")\n",
    "\n",
    "baseline_test_wo_judge = get_results(baseline_test_input_wo_judge, \"baseline prompt\", \"test\")\n",
    "baseline_val_wo_judge = get_results(baseline_val_input_wo_judge, \"baseline prompt\", \"val\")\n",
    "\n",
    "enhanced_test = get_results(enhanced_test_input_w_judge, \"prompt engineering & judge\", \"test\")\n",
    "enhanced_val = get_results(enhanced_val_input_w_judge, \"prompt engineering & judge\", \"val\")\n",
    "\n",
    "enhanced_test_wo_judge = get_results(enhanced_test_input_wo_judge, \"prompt engineering\", \"test\")\n",
    "enhanced_val_wo_judge = get_results(enhanced_val_input_wo_judge, \"prompt engineering\", \"val\")\n",
    "\n",
    "all_results = pl.concat(\n",
    "    [\n",
    "        baseline_test,\n",
    "        baseline_val,\n",
    "        baseline_test_wo_judge,\n",
    "        baseline_val_wo_judge,\n",
    "        enhanced_test,\n",
    "        enhanced_val,\n",
    "        enhanced_test_wo_judge,\n",
    "        enhanced_val_wo_judge,\n",
    "    ]\n",
    ")\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf35f8",
   "metadata": {},
   "source": [
    "### 2. Preprocess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ea815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Levenshtein distance\", \"delete percentage\", \"false positive percentage\"]:\n",
    "    all_results = all_results.with_columns((1 / pl.col(metric)).alias(f\"1/{metric}\")).drop(metric)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ac008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\n",
    "    \"cosine similarity\",\n",
    "    \"coverage percentage\",\n",
    "    \"1/Levenshtein distance\",\n",
    "    \"1/delete percentage\",\n",
    "    \"1/false positive percentage\",\n",
    "]:\n",
    "    all_results = all_results.with_columns(\n",
    "        (pl.col(metric) * pl.col(\"micro_f1_spans\")).alias(f\"weighted {metric}\")\n",
    "    ).drop(metric)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = all_results.group_by(\"experiment\", \"set\").mean()\n",
    "display(agg_results.filter(pl.col(\"set\") == \"test\").sort(\"experiment\"))#.select(\"experiment\", \"micro_f1_objects\", \"micro_f1_spans\", \"map_50\", \"weighted 1/Levenshtein distance\"))\n",
    "display(agg_results.filter(pl.col(\"set\") == \"val\").sort(\"experiment\"))#.select(\"experiment\", \"micro_f1_objects\", \"micro_f1_spans\", \"map_50\", \"weighted 1/Levenshtein distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(\n",
    "    all_results,\n",
    "    x=\"set\",\n",
    "    y=\"micro_f1_objects\",\n",
    "    color=\"experiment\",\n",
    "    color_discrete_sequence=COLORS,\n",
    "    labels={\"set\": \"Dataset\", \"micro_f1_objects\": \"Metric Value\", \"experiment\": \"Pipeline Type\"},\n",
    "    title=\"Comparison of Object-Level Micro F1 Scores\",\n",
    ").update_traces(marker=dict(size=18))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(\n",
    "    all_results,\n",
    "    x=\"set\",\n",
    "    y=\"map_50\",\n",
    "    color=\"experiment\",\n",
    "    color_discrete_sequence=COLORS,\n",
    "    labels={\"set\": \"Dataset\", \"map_50\": \"Metric Value\", \"experiment\": \"Pipeline Type\"},\n",
    "    title=\"Comparison of mAP@50 Scores\",\n",
    ").update_traces(marker=dict(size=18))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d890c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(\n",
    "    all_results,\n",
    "    x=\"set\",\n",
    "    y=\"weighted 1/Levenshtein distance\",\n",
    "    color=\"experiment\",\n",
    "    color_discrete_sequence=COLORS,\n",
    "    labels={\"set\": \"Dataset\", \"weighted 1/Levenshtein distance\": \"Metric Value\", \"experiment\": \"Pipeline Type\"},\n",
    "    title=\"Comparison of weighted 1/Levenshtein distance Scores\",\n",
    ").update_traces(marker=dict(size=18))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
