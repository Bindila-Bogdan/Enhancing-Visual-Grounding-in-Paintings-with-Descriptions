{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge annotations\n",
    "This notebook is used for the development of the LLM-as-a-judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pprint import pprint\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLM-as-a-judge for the object descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judge_llm_client():\n",
    "    with open(\"../../config/keys.json\", \"r\") as file:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = json.load(file)[\"openai_api_key\"]\n",
    "\n",
    "    return OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreExplanation(BaseModel):\n",
    "    score: int\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionScoreEvaluation(BaseModel):\n",
    "    factual_accuracy: ScoreExplanation\n",
    "    coherence: ScoreExplanation\n",
    "    grounding_potential: ScoreExplanation\n",
    "    completeness: ScoreExplanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionScore(BaseModel):\n",
    "    factual_accuracy: int\n",
    "    coherence: int\n",
    "    grounding_potential: int\n",
    "    completeness: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_objects_descriptions(client):\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of object descriptions from paintings generated by a language model. You will be given the following:\n",
    "\n",
    "    1.  **Object Name:** The name of the object.\n",
    "    2.  **Original Description Spans:** The text spans from which the object description was generated.\n",
    "    3.  **Generated Description:** The description created by the language model.\n",
    "\n",
    "    Your task is to evaluate the generated description based on the following criteria, providing a score (1-5) and a brief justification for each:\n",
    "\n",
    "    **Evaluation Criteria:**\n",
    "\n",
    "    *   **Factual Accuracy (1-5):**  Does the generated description accurately reflect the information provided in the original description spans? Does it avoid hallucination or the addition of information not present in the spans? (1 = Completely inaccurate, 5 = Perfectly accurate)\n",
    "    *   **Coherence (1-5):** Is the generated description well-written and easy to understand? Does it flow logically and make sense as a complete description? (1 = Incoherent and confusing, 5 = Perfectly coherent and clear)\n",
    "    *   **Grounding Potential (1-5):** How suitable is the generated description for use with a visual grounding model? Does it focus on visual attributes and provide specific details that would help a grounding model locate the object in an image? (1 = Very poor for grounding, 5 = Excellent for grounding)\n",
    "    *   **Completeness (1-5):** Does the description include all the information that is provided in the spans? (1 = Very poor completeness, 5 = Perfect completeness)\"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"Object Name: hawk\n",
    "\n",
    "    Original Description Spans:\n",
    "    - hawk contemplating itself in a mirror\n",
    "    - the hawk represents the material world\n",
    "    - just as the bird \"watches itself in a glass, waiting for the image to move so as to know which is really alive, itself or the image\"\n",
    "    - painted the bird from an Egyptian carving. Thus the \"real\" hawk is immobile and the flying hawk in the mirror is an illusion. The carving from which the hawk was painted now adorns the artist's grave.\n",
    "\n",
    "    Generated Description: A hawk is depicted contemplating itself in a mirror. The hawk represents the material world, and it is described as watching itself in a glass, waiting for the image to move in order to know which is really alive, itself or the image. The artist painted the bird from an Egyptian carving, which makes the \"real\" hawk immobile while the flying hawk in the mirror is an illusion. The carving from which the hawk was painted now adorns the artist's grave.\"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=0,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format=DescriptionScore,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_client = get_judge_llm_client()\n",
    "response = judge_objects_descriptions(judge_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = response.choices[0].message\n",
    "\n",
    "if message.parsed:\n",
    "    description_evaluation = dict(response.choices[0].message.parsed)\n",
    "    pprint(description_evaluation)\n",
    "else:\n",
    "    print(message.refusal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LLM-as-a-judge for extracted objects and their description spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionObjectIssues(BaseModel):\n",
    "    object_name: str\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssueDescription(BaseModel):\n",
    "    span: str\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionSpanIssues(BaseModel):\n",
    "    object_name: str\n",
    "    spans_with_issue: list[IssueDescription]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionExtractionEvaluation(BaseModel):\n",
    "    false_positive_objects: list[DescriptionObjectIssues]\n",
    "    false_negative_objects: list[DescriptionObjectIssues]\n",
    "    false_positive_objects: list[DescriptionSpanIssues]\n",
    "    false_negative_objects: list[DescriptionSpanIssues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanIssues(BaseModel):\n",
    "    object_name: str\n",
    "    spans_with_issue: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractionEvaluation(BaseModel):\n",
    "    false_positive_objects: list[str]\n",
    "    false_negative_objects: list[str]\n",
    "    false_positive_spans: list[SpanIssues]\n",
    "    false_negative_spans: list[SpanIssues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_url(image_bytes):\n",
    "    image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    image_url = f\"data:image/png;base64,{image_base64}\"\n",
    "\n",
    "    return image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_objects_extractions(client, image_data_url):\n",
    "    system_prompt = \"\"\"You are an expert art analyst tasked with evaluating the accuracy of object extraction from paintings and their corresponding textual description spans. You will be given the following:\n",
    "    \n",
    "    **Input Format**\n",
    "    1. A painting image\n",
    "    2. The original textual description of the painting\n",
    "    3. The AI system's output listing:\n",
    "        - Objects detected in both the painting and description\n",
    "        - Corresponding description spans for each object (if available, as an object can be only present in the description without being described)\n",
    "\n",
    "    **Task**\n",
    "    Your task is to evaluate each object extracted by the first LLM and check if it is mentioned in the description and appears in the painting. If it is not present in both, add it to the list of false positive. If the object appears in painting and description, but it was not extracted by the first LLM, add it to the list of false negatives. \n",
    "    After that, analyze for each object extracted by the first LLM the extracted description spans. They have to be extracted 100% accurately from the initial description. If a description span is not 100% from the description or does not describe the associated object, add it to the list of false positives. If a span describes the associated object, but is not extracted, please add it to the list of false negatives.\"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"Painting description:\n",
    "    [painting description]\n",
    "\n",
    "    Extracted objects together with their description spans:\n",
    "    - [extracted object 1]: [span 1, span 2, ...]\n",
    "    - [extracted object 2]: [span 1, span 2, ...]\"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=0,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_url}},\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=ExtractionEvaluation,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
