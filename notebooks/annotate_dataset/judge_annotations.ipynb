{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge annotations\n",
    "This notebook is used for the development of the LLM-as-a-judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLM-as-a-judge for the object descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreExplanation(BaseModel):\n",
    "    score : int\n",
    "    explanation : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionScoreEvaluation(BaseModel):\n",
    "    factual_accuracy: ScoreExplanation\n",
    "    coherence: ScoreExplanation\n",
    "    grounding_potential : ScoreExplanation\n",
    "    completeness : ScoreExplanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionScore(BaseModel):\n",
    "    factual_accuracy: int\n",
    "    coherence: int\n",
    "    grounding_potential : int\n",
    "    completeness : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judge_llm_client():\n",
    "    with open(\"../../config/keys.json\", \"r\") as file:\n",
    "        os.environ['OPENAI_API_KEY'] = json.load(file)[\"openai_api_key\"]\n",
    "\n",
    "    return OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_output(client):\n",
    "    system_prompt = \"\"\"You are an expert evaluator assessing the quality of object descriptions from paintings generated by a language model. You will be given the following:\n",
    "\n",
    "    1.  **Object Name:** The name of the object.\n",
    "    2.  **Original Description Spans:** The text spans from which the object description was generated.\n",
    "    3.  **Generated Description:** The description created by the language model.\n",
    "\n",
    "    Your task is to evaluate the generated description based on the following criteria, providing a score (1-5) and a brief justification for each:\n",
    "\n",
    "    **Evaluation Criteria:**\n",
    "\n",
    "    *   **Factual Accuracy (1-5):**  Does the generated description accurately reflect the information provided in the original description spans? Does it avoid hallucination or the addition of information not present in the spans? (1 = Completely inaccurate, 5 = Perfectly accurate)\n",
    "    *   **Coherence (1-5):** Is the generated description well-written and easy to understand? Does it flow logically and make sense as a complete description? (1 = Incoherent and confusing, 5 = Perfectly coherent and clear)\n",
    "    *   **Grounding Potential (1-5):** How suitable is the generated description for use with a visual grounding model? Does it focus on visual attributes and provide specific details that would help a grounding model locate the object in an image? (1 = Very poor for grounding, 5 = Excellent for grounding)\n",
    "    *   **Completeness (1-5):** Does the description include all the information that is provided in the spans? (1 = Very poor completeness, 5 = Perfect completeness)\"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"Object Name: hawk\n",
    "\n",
    "    Original Description Spans:\n",
    "    - hawk contemplating itself in a mirror\n",
    "    - the hawk represents the material world\n",
    "    - just as the bird \"watches itself in a glass, waiting for the image to move so as to know which is really alive, itself or the image\"\n",
    "    - painted the bird from an Egyptian carving. Thus the \"real\" hawk is immobile and the flying hawk in the mirror is an illusion. The carving from which the hawk was painted now adorns the artist's grave.\n",
    "\n",
    "    Generated Description: A hawk is depicted contemplating itself in a mirror. The hawk represents the material world, and it is described as watching itself in a glass, waiting for the image to move in order to know which is really alive, itself or the image. The artist painted the bird from an Egyptian carving, which makes the \"real\" hawk immobile while the flying hawk in the mirror is an illusion. The carving from which the hawk was painted now adorns the artist's grave.\"\"\"\n",
    "\n",
    "    response =  client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        seed=0,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "        response_format=DescriptionScore,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_client = get_judge_llm_client()\n",
    "response = judge_output(judge_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = response.choices[0].message\n",
    "\n",
    "if message.parsed:\n",
    "    description_evaluation = dict(response.choices[0].message.parsed)\n",
    "    pprint(description_evaluation)\n",
    "else:\n",
    "    print(message.refusal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
