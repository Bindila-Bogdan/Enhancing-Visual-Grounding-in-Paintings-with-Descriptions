import os
import json

from openai import OpenAI
from pydantic import BaseModel


def get_judge_llm_client():
    with open("../../config/keys.json", "r") as file:
        os.environ["OPENAI_API_KEY"] = json.load(file)["openai_api_key"]

    return OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))


def judge_object_description(
    client, model_name, object_name, description_spans, object_description
):
    class ScoreExplanation(BaseModel):
        score: int
        explanation: str

    class DescriptionScoreEvaluation(BaseModel):
        factual_accuracy: ScoreExplanation
        coherence: ScoreExplanation
        grounding_potential: ScoreExplanation
        completeness: ScoreExplanation

    class DescriptionScore(BaseModel):
        factual_accuracy: int
        coherence: int
        grounding_potential: int
        completeness: int

    system_prompt = """You are an expert evaluator assessing the quality of object descriptions from paintings generated by a language model. You will be given the following:

1.  **Object Name:** The name of the object.
2.  **Original Description Spans:** The text spans from which the object description was generated.
3.  **Generated Description:** The description created by the language model.

Your task is to evaluate the generated description based on the following criteria, providing a score (1-5) and a brief justification for each:

**Evaluation Criteria:**

*   **Factual Accuracy (1-5):**  Does the generated description accurately reflect the information provided in the original description spans? Does it avoid hallucination or the addition of information not present in the spans? (1 = Completely inaccurate, 5 = Perfectly accurate)
*   **Coherence (1-5):** Is the generated description well-written and easy to understand? Does it flow logically and make sense as a complete description? (1 = Incoherent and confusing, 5 = Perfectly coherent and clear)
*   **Grounding Potential (1-5):** How suitable is the generated description for use with a visual grounding model? Does it focus on visual attributes and provide specific details that would help a grounding model locate the object in an image? (1 = Very poor for grounding, 5 = Excellent for grounding)
*   **Completeness (1-5):** Does the description include all the information that is provided in the spans? (1 = Very poor completeness, 5 = Perfect completeness)"""

    user_prompt = f"""Object Name: {object_name}\n\nOriginal Description Spans:\n{description_spans}\n\nGenerated Description: {object_description}"""

    response = client.beta.chat.completions.parse(
        model=model_name,
        seed=0,
        temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        response_format=DescriptionScore,
    )

    output = response.choices[0].message.parsed

    if output:
        return dict(output)
    else:
        raise Exception("Error while judging the object description")


def judge_objects_descriptions(client, model_name, llm_output):
    judge_outputs = []

    for object_data in llm_output:
        description_spans = object_data.__dict__["description_spans"]

        if len(description_spans) == 0:
            judge_outputs.append({})
            continue

        object_name = object_data.__dict__["object_name"]
        formatted_description_spans = "- " + "\n- ".join(description_spans)
        object_description = object_data.__dict__["object_description"]

        judge_outputs.append(
            judge_object_description(
                client, model_name, object_name, formatted_description_spans, object_description
            )
        )

    return judge_outputs
