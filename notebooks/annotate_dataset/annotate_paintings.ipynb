{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import *\n",
    "from call_llm import *\n",
    "from ground_objects import *\n",
    "from compute_metrics import *\n",
    "from judge_annotations import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device type\n",
    "device = get_device()\n",
    "\n",
    "# load models\n",
    "llm_client = get_llm_client()\n",
    "judge_client = get_judge_llm_client()\n",
    "grounding_processor, grounding_model = get_grounding_model(device)\n",
    "sentence_similarity_model = get_sentence_similarity_model()\n",
    "\n",
    "# load data\n",
    "paintings_data, annotations, few_shot_examples, test_paintings = load_data()\n",
    "\n",
    "# if an image is not included, it doesn't have annotations\n",
    "ground_truth_bboxes, labels_to_ids = get_bbox_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with annotation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects(\n",
    "    llm_client, few_shot_examples, image, painting, total_token_count, total_token_count_judge\n",
    "):\n",
    "    feedback = None\n",
    "    object_extraction_trials_no = 2\n",
    "    unprocessed_painting_id = None\n",
    "    painting_id_to_check = None\n",
    "\n",
    "    while object_extraction_trials_no > 0:\n",
    "        llm_output, token_count, prompt_parts, reponse_parts = generate(\n",
    "            llm_client,\n",
    "            few_shot_examples,\n",
    "            image,\n",
    "            painting[\"description\"],\n",
    "            None,\n",
    "            f\"{PROMPT_TYPE}_object_extraction\",\n",
    "            feedback,\n",
    "        )\n",
    "        total_token_count += token_count\n",
    "\n",
    "        if llm_output is None:\n",
    "            unprocessed_painting_id = painting[\"painting_id\"]\n",
    "            break\n",
    "\n",
    "        llm_output = sort_and_clean_output(llm_output, painting)\n",
    "        object_and_spans = {\"object_names\": [], \"descriptions_spans\": []}\n",
    "\n",
    "        for llm_output_ in llm_output:\n",
    "            object_and_spans[\"object_names\"].append(llm_output_.object_name)\n",
    "            object_and_spans[\"descriptions_spans\"].append(llm_output_.description_spans)\n",
    "\n",
    "        if not JUDGE_OUTPUT:\n",
    "            object_extraction_trials_no = 0\n",
    "            break\n",
    "\n",
    "        object_extraction_suggestions, _, token_count_judge = judge_objects_extractions(\n",
    "            judge_client, image, painting[\"description\"], object_and_spans\n",
    "        )\n",
    "        total_token_count_judge += token_count_judge\n",
    "        object_extraction_trials_no -= 1\n",
    "\n",
    "        if len(object_extraction_suggestions) != 0 and object_extraction_trials_no <= 0:\n",
    "            painting_id_to_check = painting[\"painting_id\"]\n",
    "        elif len(object_extraction_suggestions) != 0:\n",
    "            feedback = [prompt_parts, reponse_parts, object_extraction_suggestions]\n",
    "        else:\n",
    "            object_extraction_trials_no = 0\n",
    "\n",
    "    return (\n",
    "        llm_output,\n",
    "        object_and_spans,\n",
    "        unprocessed_painting_id,\n",
    "        painting_id_to_check,\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_object_description(\n",
    "    llm_client,\n",
    "    few_shot_examples,\n",
    "    object_and_spans,\n",
    "    object_description_metrics,\n",
    "    painting_id,\n",
    "    total_token_count,\n",
    "    total_token_count_judge,\n",
    "):\n",
    "    feedback = None\n",
    "    object_description_trials_no = 2\n",
    "    unprocessed_painting_id = None\n",
    "    painting_id_to_check = None\n",
    "\n",
    "    while object_description_trials_no > 0:\n",
    "        llm_output, token_count, prompt_parts, reponse_parts = generate(\n",
    "            llm_client,\n",
    "            few_shot_examples,\n",
    "            None,\n",
    "            None,\n",
    "            object_and_spans,\n",
    "            f\"{PROMPT_TYPE}_create_description\",\n",
    "            feedback,\n",
    "        )\n",
    "        total_token_count += token_count\n",
    "\n",
    "        if llm_output is None:\n",
    "            unprocessed_painting_id = painting_id\n",
    "            break\n",
    "\n",
    "        if not JUDGE_OUTPUT:\n",
    "            object_description_trials_no = 0\n",
    "            break\n",
    "\n",
    "        object_and_spans[\"objects_description\"] = [\n",
    "            annotation.object_description for annotation in llm_output\n",
    "        ]\n",
    "        object_description_suggestions, token_count_judge = judge_objects_descriptions(\n",
    "            judge_client, object_and_spans, object_description_metrics\n",
    "        )\n",
    "        total_token_count_judge += token_count_judge\n",
    "        object_description_trials_no -= 1\n",
    "\n",
    "        if len(object_description_suggestions) != 0 and object_description_trials_no <= 0:\n",
    "            painting_id_to_check = painting_id\n",
    "        elif len(object_description_suggestions) != 0:\n",
    "            feedback = [prompt_parts, reponse_parts, object_description_suggestions]\n",
    "        else:\n",
    "            object_description_trials_no = 0\n",
    "\n",
    "    return (\n",
    "        llm_output,\n",
    "        unprocessed_painting_id,\n",
    "        painting_id_to_check,\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object metrics\n",
    "micro_f1_objects = None\n",
    "tp_fp_fn_objects = [0, 0, 0]\n",
    "all_predicted_objects = []\n",
    "all_ground_truth_objects = []\n",
    "\n",
    "# spans metrics\n",
    "micro_f1_spans = None\n",
    "tp_fp_fn_spans = [0, 0, 0]\n",
    "span_similarity_metrics = {\n",
    "    \"cosine similarity\": [],\n",
    "    \"Levenshtein distance\": [],\n",
    "    \"delete percentage\": [],\n",
    "    \"false positive percentage\": [],\n",
    "    \"coverage percentage\": [],\n",
    "}\n",
    "all_predicted_spans = []\n",
    "all_ground_truth_spans = []\n",
    "\n",
    "# object descriptions metrics\n",
    "object_description_metrics = {\n",
    "    \"factual_accuracy\": [],\n",
    "    \"coherence\": [],\n",
    "    \"grounding_potential\": [],\n",
    "    \"completeness\": [],\n",
    "}\n",
    "all_predicted_object_descriptions = []\n",
    "\n",
    "# grounding metrics\n",
    "map_50 = None\n",
    "map_50_95 = None\n",
    "all_predicted_bboxes = []\n",
    "all_ground_truth_bboxes = []\n",
    "\n",
    "# other tracked info\n",
    "total_token_count = 0\n",
    "total_token_count_judge = 0\n",
    "painting_ids = []\n",
    "paintings_ids_tp_check = []\n",
    "unprocessed_painting_ids = []\n",
    "\n",
    "for painting in tqdm(test_paintings[1:3]):\n",
    "    image = load_image(painting[\"painting_id\"])\n",
    "\n",
    "    # extract described objects\n",
    "    (\n",
    "        llm_output_objects,\n",
    "        object_and_spans,\n",
    "        unprocessed_painting_id,\n",
    "        painting_id_to_check,\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    ) = extract_objects(\n",
    "        llm_client, few_shot_examples, image, painting, total_token_count, total_token_count_judge\n",
    "    )\n",
    "\n",
    "    if unprocessed_painting_id is not None:\n",
    "        unprocessed_painting_ids.append(unprocessed_painting_id)\n",
    "        continue\n",
    "\n",
    "    if painting_id_to_check is not None:\n",
    "        paintings_ids_tp_check.append(painting_id_to_check)\n",
    "\n",
    "    # create description per object\n",
    "    (\n",
    "        llm_output_descriptions,\n",
    "        unprocessed_painting_id,\n",
    "        painting_id_to_check,\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    ) = compose_object_description(\n",
    "        llm_client,\n",
    "        few_shot_examples,\n",
    "        copy.deepcopy(object_and_spans),\n",
    "        object_description_metrics,\n",
    "        painting[\"painting_id\"],\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    )\n",
    "\n",
    "    if unprocessed_painting_id is not None:\n",
    "        unprocessed_painting_ids.append(unprocessed_painting_id)\n",
    "        continue\n",
    "\n",
    "    if painting_id_to_check is not None:\n",
    "        paintings_ids_tp_check.append(painting_id_to_check)\n",
    "\n",
    "    # handle objects\n",
    "    predicted_objects, ground_truth_objects = process_objects(\n",
    "        llm_output_objects, painting, all_predicted_objects, all_ground_truth_objects\n",
    "    )\n",
    "    count_tp_fp_fn(predicted_objects, ground_truth_objects, tp_fp_fn_objects)\n",
    "\n",
    "    # handle spans\n",
    "    (\n",
    "        predicted_spans_per_object,\n",
    "        ground_truth_spans_per_object,\n",
    "        predicted_spans,\n",
    "        ground_truth_spans,\n",
    "    ) = process_spans(llm_output_objects, painting)\n",
    "    compute_spans_quality(\n",
    "        ground_truth_spans_per_object,\n",
    "        predicted_spans_per_object,\n",
    "        span_similarity_metrics,\n",
    "        sentence_similarity_model,\n",
    "    )\n",
    "    count_tp_fp_fn(predicted_spans, ground_truth_spans, tp_fp_fn_spans)\n",
    "    all_predicted_spans.append(predicted_spans_per_object)\n",
    "    all_ground_truth_spans.append(ground_truth_spans_per_object)\n",
    "\n",
    "    # handle object description\n",
    "    get_object_descriptions(llm_output_descriptions, all_predicted_object_descriptions)\n",
    "\n",
    "    # ground objects\n",
    "    labels_scores_boxes, grounding_results = detect_objects(\n",
    "        image,\n",
    "        predicted_objects,\n",
    "        grounding_processor,\n",
    "        grounding_model,\n",
    "        device,\n",
    "        object_threshold=0.3,\n",
    "        text_threshold=0.3,\n",
    "    )\n",
    "\n",
    "    get_bounding_boxes(\n",
    "        labels_scores_boxes,\n",
    "        labels_to_ids,\n",
    "        ground_truth_bboxes,\n",
    "        painting[\"painting_id\"],\n",
    "        all_predicted_bboxes,\n",
    "        all_ground_truth_bboxes,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # if this line has been reached, the full image processing is done\n",
    "    painting_ids.append(painting[\"painting_id\"])\n",
    "\n",
    "# compute metrics across the entire dataset\n",
    "micro_f1_objects = compute_micro_f1(tp_fp_fn_objects, \"objects\")\n",
    "\n",
    "micro_f1_spans = compute_micro_f1(tp_fp_fn_spans, \"spans\")\n",
    "for metric in span_similarity_metrics:\n",
    "    span_similarity_metrics[metric] = np.array(span_similarity_metrics[metric]).mean()\n",
    "\n",
    "for metric in object_description_metrics:\n",
    "    object_description_metrics[metric] = np.array(object_description_metrics[metric]).mean()\n",
    "\n",
    "map_50, map_50_95 = compute_mean_average_precision(\n",
    "    all_predicted_bboxes, all_ground_truth_bboxes, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results for the tested prompt\n",
    "predictions = []\n",
    "\n",
    "assert len(all_predicted_spans) == len(all_predicted_object_descriptions)\n",
    "\n",
    "for index, painting_data in enumerate(all_predicted_spans):\n",
    "    object_descriptions = all_predicted_object_descriptions[index]\n",
    "    object_data = {}\n",
    "\n",
    "    for object_index, (object, spans) in enumerate(painting_data.items()):\n",
    "        object_data[object] = [spans, object_descriptions[object_index]]\n",
    "\n",
    "    predictions.append(object_data)\n",
    "\n",
    "results_values = list(zip(painting_ids, predictions, all_ground_truth_spans))\n",
    "\n",
    "metrics = {\n",
    "    \"total_token_count\": total_token_count,\n",
    "    \"total_token_count_judge\": total_token_count_judge,\n",
    "    \"unprocessed_painting_ids\": unprocessed_painting_ids,\n",
    "    \"paintings_ids_tp_check\": paintings_ids_tp_check,\n",
    "    \"micro_f1_objects\": micro_f1_objects,\n",
    "    \"micro_f1_spans\": micro_f1_spans,\n",
    "    \"span_similarity_metrics\": span_similarity_metrics,\n",
    "    \"object_description_metrics\": object_description_metrics,\n",
    "    \"map_50\": map_50,\n",
    "    \"map_50_95\": map_50_95,\n",
    "}\n",
    "\n",
    "observations = \"first trial with few-shot learning\"\n",
    "store_results(PROMPT_TYPE, observations, results_values, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create object-descriptions for few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_objects_description():\n",
    "    class Description(BaseModel):\n",
    "        description: str\n",
    "\n",
    "    system_prompt_text = \"\"\"You are an art expert providing detailed descriptions of objects depicted in paintings. \n",
    "    You are given an object name and a set of descriptive text spans from the painting's museum label. \n",
    "    Your task is to create a single, coherent description paragraph that starts with the object name of the object based solely on the provided information. \n",
    "    You have to included all the provided details from the description spans.\n",
    "\n",
    "    **Constraints:**\n",
    "    Do not add any details about the object that are not explicitly mentioned in the provided description spans.\n",
    "    Do not infer the object's material, purpose, or origin unless it is directly stated in the text.\n",
    "    Focus on combining and rephrasing the given information, not on creating new information.\n",
    "    Do not assume anything about the object's cultural significance or symbolism unless the provided spans mention it.\"\"\"\n",
    "\n",
    "    _, _, few_shot_examples, _ = load_data()\n",
    "\n",
    "    for few_shot_example in few_shot_examples:\n",
    "        print(few_shot_example[\"painting_id\"])\n",
    "\n",
    "        for index in range(len(few_shot_example[\"object_name\"])):\n",
    "\n",
    "            object_name = few_shot_example[\"object_name\"][index]\n",
    "            description_spans = \"- \" + \"\\n- \".join(few_shot_example[\"description_spans\"][index])\n",
    "\n",
    "            if len(description_spans) == 2:\n",
    "                print(object_name)\n",
    "                continue\n",
    "\n",
    "            prompt_parts_text = f\"\"\"**Object Name:**\\n{object_name}\\n\\n**Description Spans:**\\n{description_spans}\\n\\n**Generated Description:**\"\"\"\n",
    "\n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.0,\n",
    "                response_mime_type=\"application/json\",\n",
    "                system_instruction=[\n",
    "                    types.Part.from_text(text=system_prompt_text),\n",
    "                ],\n",
    "                response_schema=Description,\n",
    "            )\n",
    "\n",
    "            called = False\n",
    "\n",
    "            while not called:\n",
    "                try:\n",
    "                    response = llm_client.models.generate_content(\n",
    "                        model=GEMINI_MODEL,\n",
    "                        contents=prompt_parts_text,\n",
    "                        config=generate_content_config,\n",
    "                    )\n",
    "                    called = True\n",
    "                except:\n",
    "                    print(\"Try again...\")\n",
    "                    time.sleep(5)\n",
    "\n",
    "            print(object_name)\n",
    "            print(response.parsed.description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
