{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "from PIL import Image\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "GROUNDING_MODEL_ID = \"IDEA-Research/grounding-dino-base\"\n",
    "\n",
    "\n",
    "RAW_DATA_PATH = \"../../data/raw/\"\n",
    "ANNOTATIONS_PATH = \"../../data/annotations/\"\n",
    "INTERMEDIATE_DATA_PATH = \"../../data/intermediate/filtered_paintings/\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../config/keys.json\", \"r\") as file:\n",
    "    os.environ[\"GEMINI_API_KEY\"] = json.load(file)[\"gemini_api_key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract with an LLM the described objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_available_models(client):\n",
    "    existing_models = [\n",
    "        model.name.split(\"/\")[1]\n",
    "        for model in client.models.list()\n",
    "        if \"generateContent\" in model.supported_actions\n",
    "    ]\n",
    "\n",
    "    print(existing_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_bytes(image):\n",
    "    # define an in-memory byte stream\n",
    "    img_byte_array = io.BytesIO()\n",
    "\n",
    "    # convert the image to a byte representation and store it in the in-memory byte stream\n",
    "    image.save(img_byte_array, format=image.format or \"PNG\")\n",
    "\n",
    "    # get the byte representation of the image\n",
    "    img_bytes = img_byte_array.getvalue()\n",
    "\n",
    "    return img_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(client, examples, image, description):\n",
    "    # instructions = \"\"\"List separated by comma of **ONLY** the objects (lowercased) described in the given painting that also appear in the textual description. \n",
    "    # If an object is only present in the description, but there aren't details about it, do not include it in the list. Explain how your reasoning step by step. The description is:\\n\n",
    "    # \"\"\"\n",
    "    # input_prompt = instructions + f'\"{description}\"'\n",
    "\n",
    "    prompt_parts = []\n",
    "\n",
    "    prompt_parts.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=\"\\nHere are some examples:\")]))\n",
    "\n",
    "    for example in examples:\n",
    "        example_painting_id = example[\"painting_id\"]\n",
    "        example_description = example[\"description\"]\n",
    "        example_detected_objects = \", \".join(example[\"object_name\"])\n",
    "\n",
    "        example_image = Image.open(f\"{RAW_DATA_PATH}filtered_paintings/{example_painting_id}.png\")\n",
    "\n",
    "        prompt_parts.append(types.Content(role=\"user\", parts=[\n",
    "            types.Part.from_bytes(mime_type=\"image/png\", data=image_to_bytes(example_image)),\n",
    "            types.Part.from_text(text=f\"Description: \\\"{example_description}\\\"\")\n",
    "        ]))\n",
    "        prompt_parts.append(types.Content(role=\"model\", parts=[\n",
    "            types.Part.from_text(text=f\"Detected objects: {example_detected_objects}\")\n",
    "        ]))\n",
    "        prompt_parts.append(types.Part.from_text(text=\"---\"))\n",
    "\n",
    "\n",
    "    prompt_parts.append(types.Content(role=\"user\", parts=[\n",
    "        types.Part.from_bytes(mime_type=\"image/png\", data=image_to_bytes(image)),\n",
    "        types.Part.from_text(text=f\"Description: \\\"{description}\\\"\\n\\nList separated by comma of **ONLY** the objects (lowercased) described in the given painting that also appear in the textual description.\")\n",
    "    ]))\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/plain\",\n",
    "        system_instruction=[\n",
    "            types.Part.from_text(text=\"\"\"You are an expert in art who can identify objects present in both a painting and its textual description.\"\"\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=prompt_parts,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "\n",
    "    output = response.text\n",
    "    prompt_tokens_count = response.usage_metadata.prompt_token_count\n",
    "    output_tokens_count = response.usage_metadata.candidates_token_count\n",
    "\n",
    "    print(f\"Prompt tokens count: {prompt_tokens_count}\\nOutput tokens count: {output_tokens_count}\")\n",
    "    print(f\"Response:\\n{output}\\n\")\n",
    "\n",
    "    return prompt_parts, generate_content_config, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ground Objects with Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(GROUNDING_MODEL_ID)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(GROUNDING_MODEL_ID).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_annotated_image(image, labels_scores_boxes):\n",
    "    font = ImageFont.truetype(\"../../config/alata-regular.ttf\", 18)\n",
    "    draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "    for label, score, coords in labels_scores_boxes:\n",
    "        random_color = \"#{:06x}\".format(random.randint(0, 0xFFFFFF)) + \"80\"\n",
    "        text_position = (coords[0] + 10, coords[1] + 5)\n",
    "\n",
    "        draw.rectangle(coords, outline=random_color, width=5)\n",
    "        draw.text(text_position, label + \" \" + str(round(score, 2)), fill=random_color, font=font)\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image, text, processor, model, object_threshold=0.3, text_threshold=0.3):\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        # threshold for filtering object detection predictions (lower -> more bounding boxes)\n",
    "        threshold=object_threshold,\n",
    "        # threshold for filtering text detection predictions (lower -> the input text is taken exactly)\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[image.size[::-1]],\n",
    "    )\n",
    "\n",
    "    assert len(results) == 1\n",
    "\n",
    "    labels = results[0][\"text_labels\"]\n",
    "    scores = results[0][\"scores\"].cpu().numpy()\n",
    "    box_coordinates = [list(coords) for coords in results[0][\"boxes\"].cpu().numpy()]\n",
    "    labels_scores_boxes = sorted(list(zip(labels, scores, box_coordinates)), key=lambda x: x[1])\n",
    "\n",
    "    for label, score, coords in labels_scores_boxes:\n",
    "        print(label, float(score), [float(coord) for coord in coords])\n",
    "\n",
    "    display_annotated_image(image, labels_scores_boxes)\n",
    "\n",
    "    return labels_scores_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "paintings_data = pl.read_json(f\"{INTERMEDIATE_DATA_PATH}filtered_paintings_enhanced_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_example_ids = [2156, 2484, 11819, 256, 10748, 3344, 10676]\n",
    "annotations = pl.read_json(ANNOTATIONS_PATH + \"manual_annotations.json\")\n",
    "few_shots_descriptions = paintings_data.filter(pl.col(\"id\").is_in(few_shot_example_ids)).select(\"id\", \"description\").rename({\"id\": \"painting_id\"})\n",
    "\n",
    "few_shot_examples = (\n",
    "    annotations.filter(pl.col(\"painting_id\").is_in(few_shot_example_ids))\n",
    "    .group_by(\"painting_id\")\n",
    "    .agg(pl.col(\"*\"))\n",
    "    .select(\"painting_id\", \"object_name\", \"description_spans\")\n",
    ")\n",
    "\n",
    "few_shot_examples = few_shot_examples.join(few_shots_descriptions, on=\"painting_id\").to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_id = 2461\n",
    "description = paintings_data[painting_id, \"description\"]\n",
    "image = Image.open(f\"../../data/raw/filtered_paintings/{painting_id}.png\")\n",
    "\n",
    "# extarct described objects\n",
    "prompt_parts, generate_content_config, output = generate(client, few_shot_examples, image, description)\n",
    "\n",
    "# ground objects\n",
    "text = output.replace(\", \", \". \") + \".\"\n",
    "labels_scores_boxes = detect_objects(\n",
    "    image, text, processor, model, object_threshold=0.3, text_threshold=0.3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
