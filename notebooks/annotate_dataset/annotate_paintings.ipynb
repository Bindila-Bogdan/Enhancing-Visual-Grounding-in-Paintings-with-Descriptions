{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from call_llm import *\n",
    "from ground_objects import *\n",
    "from compute_metrics import *\n",
    "from judge_annotations import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash-lite\"\n",
    "OPEN_AI_MODEL = \"gpt-4.1-nano-2025-04-14\"\n",
    "SENTENCE_SIMILARITY_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "GROUNDING_MODEL_ID = \"IDEA-Research/grounding-dino-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device type\n",
    "device = get_device()\n",
    "\n",
    "# load models\n",
    "llm_client = get_llm_client()\n",
    "judge_client = get_judge_llm_client()\n",
    "grounding_processor, grounding_model = get_grounding_model(GROUNDING_MODEL_ID, device)\n",
    "sentence_similarity_model = get_sentence_similarity_model(SENTENCE_SIMILARITY_MODEL_NAME)\n",
    "\n",
    "# load data\n",
    "paintings_data, annotations, few_shot_examples, test_paintings = load_data()\n",
    "\n",
    "# if an image is not included, it doesn't have annotations\n",
    "ground_truth_bboxes, labels_to_ids = get_bbox_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with annotation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "prompt_type = \"basic_complete\"\n",
    "observations = \"first trial with few-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object metrics\n",
    "micro_f1_objects = None\n",
    "tp_fp_fn_objects = [0, 0, 0]\n",
    "all_predicted_objects = []\n",
    "all_ground_truth_objects = []\n",
    "\n",
    "# spans metrics\n",
    "micro_f1_spans = None\n",
    "tp_fp_fn_spans = [0, 0, 0]\n",
    "span_similarity_metrics = {\n",
    "    \"cosine similarity\": [],\n",
    "    \"Levenshtein distance\": [],\n",
    "    \"delete percentage\": [],\n",
    "    \"false positive percentage\": [],\n",
    "    \"coverage percentage\": [],\n",
    "}\n",
    "all_predicted_spans = []\n",
    "all_ground_truth_spans = []\n",
    "\n",
    "# object descriptions metrics\n",
    "object_description_metrics = {\n",
    "     \"factual_accuracy\": [],\n",
    "     \"coherence\": [],\n",
    "     \"grounding_potential\": [],\n",
    "     \"completeness\": []\n",
    "}\n",
    "all_predicted_object_descriptions = []\n",
    "\n",
    "# grounding metrics\n",
    "map_50 = None\n",
    "map_50_95 = None\n",
    "all_predicted_bboxes = []\n",
    "all_ground_truth_bboxes = []\n",
    "\n",
    "# other tracked info\n",
    "total_token_count = 0\n",
    "painting_ids = []\n",
    "unprocessed_painting_ids = []\n",
    "\n",
    "for painting in tqdm(test_paintings[:5]):\n",
    "    painting_ids.append(painting[\"painting_id\"])\n",
    "    image = load_image(painting[\"painting_id\"])\n",
    "\n",
    "    # extract described objects\n",
    "    llm_output, token_count = generate(\n",
    "        llm_client,\n",
    "        few_shot_examples,\n",
    "        image,\n",
    "        painting[\"description\"],\n",
    "        prompt_type,\n",
    "        GEMINI_MODEL,\n",
    "        verbose,\n",
    "    )\n",
    "    total_token_count += token_count\n",
    "\n",
    "    if llm_output is None:\n",
    "        unprocessed_painting_ids.append(painting[\"painting_id\"])\n",
    "        continue\n",
    "\n",
    "    spans_are_extracted = \"description_spans\" in llm_output[0].__dict__\n",
    "    description_is_extracted = \"object_description\" in llm_output[0].__dict__\n",
    "    sorted(llm_output, key=lambda x: x.object_name)\n",
    "\n",
    "    # handle objects\n",
    "    predicted_objects, ground_truth_objects = process_objects(\n",
    "        llm_output, painting, all_predicted_objects, all_ground_truth_objects, verbose\n",
    "    )\n",
    "    compute_f1(predicted_objects, ground_truth_objects, tp_fp_fn_objects)\n",
    "\n",
    "    # handle spans\n",
    "    if spans_are_extracted:\n",
    "        predicted_spans_per_object, ground_truth_spans_per_object, predicted_spans, ground_truth_spans = process_spans(llm_output, painting)\n",
    "        compute_spans_quality(\n",
    "            ground_truth_spans_per_object,\n",
    "            predicted_spans_per_object,\n",
    "            span_similarity_metrics,\n",
    "            sentence_similarity_model,\n",
    "            verbose\n",
    "        )\n",
    "        compute_f1(predicted_spans, ground_truth_spans, tp_fp_fn_spans)\n",
    "        all_predicted_spans.append(predicted_spans_per_object)\n",
    "        all_ground_truth_spans.append(ground_truth_spans_per_object)\n",
    "\n",
    "    # handle object description\n",
    "    if description_is_extracted:\n",
    "        get_object_descriptions(llm_output, all_predicted_object_descriptions)\n",
    "        judge_objects_descriptions(judge_client, OPEN_AI_MODEL, llm_output, object_description_metrics)\n",
    "\n",
    "    # ground objects\n",
    "    labels_scores_boxes, results = detect_objects(\n",
    "        image,\n",
    "        predicted_objects,\n",
    "        grounding_processor,\n",
    "        grounding_model,\n",
    "        device,\n",
    "        verbose,\n",
    "        object_threshold=0.3,\n",
    "        text_threshold=0.3,\n",
    "    )\n",
    "\n",
    "    get_bounding_boxes(\n",
    "        labels_scores_boxes,\n",
    "        labels_to_ids,\n",
    "        ground_truth_bboxes,\n",
    "        painting[\"painting_id\"],\n",
    "        all_predicted_bboxes,\n",
    "        all_ground_truth_bboxes,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "# compute metrics across the entire dataset\n",
    "micro_f1_objects = compute_micro_f1(tp_fp_fn_objects, \"objects\", verbose)\n",
    "\n",
    "if spans_are_extracted:\n",
    "    micro_f1_spans = compute_micro_f1(tp_fp_fn_spans, \"spans\", verbose)\n",
    "    for metric in span_similarity_metrics:\n",
    "        span_similarity_metrics[metric] = np.array(span_similarity_metrics[metric]).mean()\n",
    "\n",
    "if description_is_extracted:\n",
    "    for metric in object_description_metrics:\n",
    "        object_description_metrics[metric] = np.array(object_description_metrics[metric]).mean()\n",
    "\n",
    "map_50, map_50_95 = compute_mean_average_precision(\n",
    "    all_predicted_bboxes, all_ground_truth_bboxes, device, verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results for the tested prompt\n",
    "predictions = []\n",
    "\n",
    "for index, painting_data in enumerate(all_predicted_spans):\n",
    "    object_descriptions = all_predicted_object_descriptions[index]\n",
    "    object_data = {}\n",
    "\n",
    "    for object_index, (object, spans) in enumerate(painting_data.items()):\n",
    "        object_data[object] = [spans, object_descriptions[object_index]]\n",
    "    \n",
    "    predictions.append(object_data)\n",
    "\n",
    "results_values = list(zip(painting_ids, predictions, all_ground_truth_spans))\n",
    "\n",
    "metrics = {\n",
    "    \"total_token_count\": total_token_count,\n",
    "    \"unprocessed_painting_ids\": unprocessed_painting_ids,\n",
    "    \"micro_f1_objects\": micro_f1_objects,\n",
    "    \"micro_f1_spans\": micro_f1_spans,\n",
    "    \"span_similarity_metrics\": span_similarity_metrics,\n",
    "    \"object_description_metrics\": object_description_metrics,\n",
    "    \"map_50\": map_50,\n",
    "    \"map_50_95\": map_50_95,\n",
    "}\n",
    "\n",
    "store_results(prompt_type, observations, results_values, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: handle empty detections for a painting\n",
    "- TODO: how to treat the case when llm_output is None?\n",
    "- TODO: get and store the number of tokens processed by the judge\n",
    "- TODO: change back the LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create object-descriptions for few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Description(BaseModel):\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_text = \"\"\"You are an art expert providing detailed descriptions of objects depicted in paintings. You are given an object name and a set of descriptive text spans from the painting's museum label. Your task is to create a single, coherent description paragraph that starts with the object name of the object based solely on the provided information. You have to included all the provided details from the description spans.\n",
    "\n",
    "**Constraints:**\n",
    "Do not add any details about the object that are not explicitly mentioned in the provided description spans.\n",
    "Do not infer the object's material, purpose, or origin unless it is directly stated in the text.\n",
    "Focus on combining and rephrasing the given information, not on creating new information.\n",
    "Do not assume anything about the object's cultural significance or symbolism unless the provided spans mention it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, few_shot_examples, _ = load_data()\n",
    "\n",
    "for few_shot_example in few_shot_examples:\n",
    "    print(few_shot_example[\"painting_id\"])\n",
    "\n",
    "    for index in range(len(few_shot_example[\"object_name\"])):\n",
    "\n",
    "        object_name = few_shot_example[\"object_name\"][index]\n",
    "        description_spans = \"- \" + \"\\n- \".join(few_shot_example[\"description_spans\"][index])\n",
    "        \n",
    "        if len(description_spans) == 2:\n",
    "            print(object_name)\n",
    "            print()\n",
    "            continue\n",
    "\n",
    "        prompt_parts_text = f\"\"\"**Object Name:**\\n{object_name}\\n\\n**Description Spans:**\\n{description_spans}\\n\\n**Generated Description:**\"\"\"\n",
    "\n",
    "        generate_content_config = types.GenerateContentConfig(\n",
    "            temperature=0.0,\n",
    "            response_mime_type=\"application/json\",\n",
    "            system_instruction=[\n",
    "                types.Part.from_text(text=system_prompt_text),\n",
    "            ],\n",
    "            response_schema=Description,\n",
    "        )\n",
    "\n",
    "        called = False\n",
    "\n",
    "        while not called:\n",
    "            try:\n",
    "                response = llm_client.models.generate_content(\n",
    "                    model=GEMINI_MODEL,\n",
    "                    contents=prompt_parts_text,\n",
    "                    config=generate_content_config,\n",
    "                )\n",
    "                called = True\n",
    "            except:\n",
    "                print(\"Try again...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        print(object_name)\n",
    "        print(response.parsed.description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
