{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from call_llm import *\n",
    "from ground_objects import *\n",
    "from compute_metrics import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "SENTENCE_SIMILARITY_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "GROUNDING_MODEL_ID = \"IDEA-Research/grounding-dino-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device type\n",
    "device = get_device()\n",
    "\n",
    "# load models\n",
    "llm_client = get_llm_client()\n",
    "grounding_processor, grounding_model = get_grounding_model(GROUNDING_MODEL_ID, device)\n",
    "sentence_similarity_model = get_sentence_similarity_model(SENTENCE_SIMILARITY_MODEL_NAME)\n",
    "\n",
    "# load data\n",
    "paintings_data, annotations, few_shot_examples, test_paintings = load_data()\n",
    "\n",
    "# if an image is not included, it doesn't have annotations\n",
    "ground_truth_bboxes, labels_to_ids = get_bbox_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with annotation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "prompt_type = \"basic\"\n",
    "observations = \"first trial with few-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_fp_fn = [0, 0, 0]\n",
    "total_token_count = 0\n",
    "\n",
    "painting_ids = []\n",
    "all_predicted_objects = []\n",
    "all_ground_truth_objects = []\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for painting in tqdm(test_paintings[:1]):\n",
    "    painting_id = painting[\"painting_id\"]\n",
    "    painting_ids.append(painting_id)\n",
    "    ground_truth_objects = copy.deepcopy(painting[\"object_name\"])\n",
    "    description = copy.deepcopy(painting[\"description\"])\n",
    "    image = load_image(painting_id)\n",
    "\n",
    "    # extract described objects\n",
    "    predicted_objects, token_count = generate(\n",
    "        llm_client,\n",
    "        few_shot_examples,\n",
    "        image,\n",
    "        description,\n",
    "        prompt_type,\n",
    "        GEMINI_MODEL,\n",
    "        verbose,\n",
    "    )\n",
    "    total_token_count += token_count\n",
    "\n",
    "    # compute metrics\n",
    "    predicted_objects, ground_truth_objects = clean_labels(predicted_objects, ground_truth_objects)\n",
    "    all_predicted_objects.append(predicted_objects)\n",
    "    all_ground_truth_objects.append(ground_truth_objects)\n",
    "\n",
    "    if verbose:\n",
    "        print(predicted_objects, ground_truth_objects)\n",
    "\n",
    "    compute_f1(copy.deepcopy(predicted_objects), copy.deepcopy(ground_truth_objects), tp_fp_fn)\n",
    "\n",
    "    # ground objects\n",
    "    labels_scores_boxes, results = detect_objects(\n",
    "        image,\n",
    "        predicted_objects,\n",
    "        grounding_processor,\n",
    "        grounding_model,\n",
    "        device,\n",
    "        verbose,\n",
    "        object_threshold=0.3,\n",
    "        text_threshold=0.3,\n",
    "    )\n",
    "\n",
    "    prediction, target = get_bounding_boxes(\n",
    "        labels_scores_boxes, labels_to_ids, ground_truth_bboxes, painting_id, device\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    targets.append(target)\n",
    "\n",
    "\n",
    "micro_f1 = compute_micro_f1(tp_fp_fn, verbose)\n",
    "compute_mean_average_precision = compute_mean_average_precision(predictions, targets, device, verbose)\n",
    "print(f\"Total token count: {total_token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results for the tested prompt\n",
    "results_values = list(zip(painting_ids, all_predicted_objects, all_ground_truth_objects))\n",
    "store_results(micro_f1, results_values, prompt_type, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_span = \"The c sat a on the mat x.\"\n",
    "extracted_span = \"The cat sat on the mat.\"\n",
    "span_extraction_metrics = compare_spans(\n",
    "    ground_truth_span, extracted_span, sentence_similarity_model, verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compute the F1 score for spans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
