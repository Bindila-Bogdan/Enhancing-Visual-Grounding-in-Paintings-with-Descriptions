{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from call_llm import *\n",
    "from ground_objects import *\n",
    "from compute_metrics import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "SENTENCE_SIMILARITY_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "GROUNDING_MODEL_ID = \"IDEA-Research/grounding-dino-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device type\n",
    "device = get_device()\n",
    "\n",
    "# load models\n",
    "llm_client = get_llm_client()\n",
    "grounding_processor, grounding_model = get_grounding_model(GROUNDING_MODEL_ID, device)\n",
    "sentence_similarity_model = get_sentence_similarity_model(SENTENCE_SIMILARITY_MODEL_NAME)\n",
    "\n",
    "# load data\n",
    "paintings_data, annotations, few_shot_examples, test_paintings = load_data()\n",
    "\n",
    "# if an image is not included, it doesn't have annotations\n",
    "ground_truth_bboxes, labels_to_ids = get_bbox_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with annotation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "prompt_type = \"basic_with_spans\"\n",
    "observations = \"first trial with few-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle empty detections for a painting\n",
    "# TODO: store everything that's needed inside the results file (e.g. spans)\n",
    "# TODO: handle if the LLM output is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_fp_fn_objects = [0, 0, 0]\n",
    "tp_fp_fn_spans = [0, 0, 0]\n",
    "total_token_count = 0\n",
    "\n",
    "painting_ids = []\n",
    "\n",
    "all_predicted_objects = []\n",
    "all_ground_truth_objects = []\n",
    "\n",
    "all_predicted_bboxes = []\n",
    "all_ground_truth_bboxes = []\n",
    "\n",
    "\n",
    "span_similarity_metrics = {\n",
    "    \"cosine similarity\": [],\n",
    "    \"Levenshtein distance\": [],\n",
    "    \"delete percentage\": [],\n",
    "    \"false positive percentage\": [],\n",
    "    \"coverage percentage\": [],\n",
    "}\n",
    "\n",
    "for painting in tqdm(test_paintings[:1]):\n",
    "    painting_id = painting[\"painting_id\"]\n",
    "    painting_ids.append(painting_id)\n",
    "\n",
    "    description = painting[\"description\"]\n",
    "    image = load_image(painting_id)\n",
    "\n",
    "    # extract described objects\n",
    "    llm_output, token_count = generate(\n",
    "        llm_client,\n",
    "        few_shot_examples,\n",
    "        image,\n",
    "        description,\n",
    "        prompt_type,\n",
    "        GEMINI_MODEL,\n",
    "        verbose,\n",
    "    )\n",
    "    total_token_count += token_count\n",
    "    spans_are_extracted = \"description_spans\" in llm_output[0].__dict__\n",
    "\n",
    "    # handle objects\n",
    "    predicted_objects, ground_truth_objects = process_objects(\n",
    "        llm_output, painting, all_predicted_objects, all_ground_truth_objects, verbose\n",
    "    )\n",
    "    compute_f1(predicted_objects, ground_truth_objects, tp_fp_fn_objects)\n",
    "\n",
    "    # handle spans\n",
    "    if spans_are_extracted:\n",
    "        predicted_spans_per_object, ground_truth_spans_per_object, predicted_spans, ground_truth_spans = process_spans(llm_output, painting)\n",
    "        compute_spans_quality(\n",
    "            ground_truth_spans_per_object,\n",
    "            predicted_spans_per_object,\n",
    "            span_similarity_metrics,\n",
    "            sentence_similarity_model,\n",
    "            verbose\n",
    "        )\n",
    "        compute_f1(predicted_spans, ground_truth_spans, tp_fp_fn_spans)\n",
    "\n",
    "\n",
    "    # ground objects\n",
    "    labels_scores_boxes, results = detect_objects(\n",
    "        image,\n",
    "        predicted_objects,\n",
    "        grounding_processor,\n",
    "        grounding_model,\n",
    "        device,\n",
    "        verbose,\n",
    "        object_threshold=0.3,\n",
    "        text_threshold=0.3,\n",
    "    )\n",
    "\n",
    "    get_bounding_boxes(\n",
    "        labels_scores_boxes,\n",
    "        labels_to_ids,\n",
    "        ground_truth_bboxes,\n",
    "        painting_id,\n",
    "        all_predicted_bboxes,\n",
    "        all_ground_truth_bboxes,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "micro_f1_objects = compute_micro_f1(tp_fp_fn_objects, \"objects\", verbose)\n",
    "\n",
    "if spans_are_extracted:\n",
    "    micro_f1_spans = compute_micro_f1(tp_fp_fn_spans, \"spans\", verbose)\n",
    "    for metric in span_similarity_metrics:\n",
    "        span_similarity_metrics[metric] = np.array(span_similarity_metrics[metric]).mean()\n",
    "\n",
    "map_50, map_50_95 = compute_mean_average_precision(\n",
    "    all_predicted_bboxes, all_ground_truth_bboxes, device, verbose\n",
    ")\n",
    "print(f\"Total token count: {total_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results for the tested prompt\n",
    "# results_values = list(zip(painting_ids, all_predicted_objects, all_ground_truth_objects))\n",
    "# store_results(micro_f1, results_values, prompt_type, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compute the F1 score for spans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
