{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "from PIL import Image\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "GROUNDING_MODEL_ID = \"IDEA-Research/grounding-dino-base\"\n",
    "\n",
    "\n",
    "RAW_DATA_PATH = \"../../data/raw/\"\n",
    "INTERMEDIATE_DATA_PATH = \"../../data/intermediate/filtered_paintings/\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../config/keys.json\", \"r\") as file:\n",
    "    os.environ['GEMINI_API_KEY'] = json.load(file)[\"gemini_api_key\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract with an LLM the described objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_available_models(client):\n",
    "    existing_models = [\n",
    "        model.name.split(\"/\")[1]\n",
    "        for model in client.models.list()\n",
    "        if \"generateContent\" in model.supported_actions\n",
    "    ]\n",
    "\n",
    "    print(existing_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_bytes(image):\n",
    "    # define an in-memory byte stream\n",
    "    img_byte_array = io.BytesIO()\n",
    "\n",
    "    # convert the image to a byte representation and store it in the in-memory byte stream\n",
    "    image.save(img_byte_array, format=image.format)\n",
    "\n",
    "    # get the byte representation of the image\n",
    "    img_bytes = img_byte_array.getvalue()\n",
    "    \n",
    "    return img_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(client, image, description):\n",
    "    instructions = \"\"\"List separated by comma of **ONLY** the objects (lowercased) described in the given painting that also appear in the textual description. \n",
    "    If an object is only present in the description, but there aren't details about it, do not include it in the list. \n",
    "    Do not output anything else besides this list. If the list is empty output only no. The description is:\\n\n",
    "    \"\"\"\n",
    "    input_prompt = instructions + f'\"{description}\"'\n",
    "\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_bytes(mime_type=\"\"\"image/png\"\"\", data=image_to_bytes(image)),\n",
    "                types.Part.from_text(text=input_prompt),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/plain\",\n",
    "        system_instruction=[\n",
    "            types.Part.from_text(text=\"\"\"You are an expert in art.\"\"\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "\n",
    "    output = response.text\n",
    "    prompt_tokens_count = response.usage_metadata.prompt_token_count\n",
    "    output_tokens_count = response.usage_metadata.candidates_token_count\n",
    "\n",
    "    print(f\"Prompt tokens count: {prompt_tokens_count}\\nOutput tokens count: {output_tokens_count}\")\n",
    "    print(f\"Response:\\n{output}\\n\")\n",
    "\n",
    "    return input_prompt, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ground Objects with Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(GROUNDING_MODEL_ID)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(GROUNDING_MODEL_ID).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_annotated_image(image, labels_scores_boxes):\n",
    "    font = ImageFont.truetype(\"../../config/alata-regular.ttf\", 18)\n",
    "    draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "    for label, score, coords in labels_scores_boxes:\n",
    "        random_color = \"#{:06x}\".format(random.randint(0, 0xFFFFFF)) + \"80\"\n",
    "        text_position = (coords[0] + 10, coords[1] + 5)\n",
    "\n",
    "        draw.rectangle(coords, outline=random_color, width=5)\n",
    "        draw.text(text_position, label + \" \" + str(round(score, 2)), fill=random_color, font=font)\n",
    "        \n",
    "    display(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image, text, processor, model, object_threshold=0.3, text_threshold=0.3):\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        # threshold for filtering object detection predictions (lower -> more bounding boxes)\n",
    "        threshold=object_threshold,\n",
    "        # threshold for filtering text detection predictions (lower -> the input text is taken exactly)\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[image.size[::-1]],\n",
    "    )\n",
    "\n",
    "    assert len(results) == 1\n",
    "\n",
    "    labels = results[0][\"text_labels\"]\n",
    "    scores = results[0][\"scores\"].cpu().numpy()\n",
    "    box_coordinates = [list(coords) for coords in results[0][\"boxes\"].cpu().numpy()]\n",
    "    labels_scores_boxes = sorted(list(zip(labels, scores, box_coordinates)), key=lambda x: x[1])\n",
    "\n",
    "    for label, score, coords in labels_scores_boxes:\n",
    "        print(label, float(score), [float(coord) for coord in coords])\n",
    "        \n",
    "    display_annotated_image(image, labels_scores_boxes)\n",
    "\n",
    "    return labels_scores_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "paintings_data = pl.read_json(f\"{INTERMEDIATE_DATA_PATH}filtered_paintings_enhanced_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_id = 11819\n",
    "description = paintings_data[painting_id, \"description\"]\n",
    "image = Image.open(f\"../../data/raw/filtered_paintings/{painting_id}.png\")\n",
    "\n",
    "# extarct described objects\n",
    "input_prompt, output = generate(client, image, description)\n",
    "\n",
    "# ground objects\n",
    "text = output.replace(\", \", \". \") + \".\"\n",
    "labels_scores_boxes = detect_objects(image, text, processor, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
