{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import *\n",
    "from call_llm import *\n",
    "from ground_objects import *\n",
    "from compute_metrics import *\n",
    "from judge_annotations import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects(\n",
    "    llm_client,\n",
    "    judge_client,\n",
    "    few_shot_examples,\n",
    "    image,\n",
    "    object_extraction_metrics,\n",
    "    painting,\n",
    "    total_token_count,\n",
    "    total_token_count_judge,\n",
    "):\n",
    "    feedback = None\n",
    "    object_extraction_trials_no = 2\n",
    "    unprocessed_painting_id = None\n",
    "    painting_id_to_check = None\n",
    "    painting_id_wo_objects = None\n",
    "\n",
    "    while object_extraction_trials_no > 0:\n",
    "        llm_output, token_count, prompt_parts, reponse_parts = generate(\n",
    "            llm_client,\n",
    "            few_shot_examples,\n",
    "            image,\n",
    "            painting[\"description\"],\n",
    "            None,\n",
    "            f\"{PROMPT_TYPE}_object_extraction\",\n",
    "            feedback,\n",
    "        )\n",
    "        total_token_count += token_count\n",
    "\n",
    "        if llm_output is None:\n",
    "            object_and_spans = None\n",
    "            unprocessed_painting_id = painting[\"painting_id\"]\n",
    "            break\n",
    "\n",
    "        llm_output = sort_and_clean_output(llm_output, painting)\n",
    "\n",
    "        if VERBOSE:\n",
    "            pprint(llm_output)\n",
    "\n",
    "        if len(llm_output) == 0:\n",
    "            painting_id_wo_objects = painting[\"painting_id\"]\n",
    "            object_and_spans = None\n",
    "            break\n",
    "\n",
    "        object_and_spans = {\"object_names\": [], \"descriptions_spans\": []}\n",
    "\n",
    "        for llm_output_ in llm_output:\n",
    "            object_and_spans[\"object_names\"].append(llm_output_.object_name)\n",
    "            object_and_spans[\"descriptions_spans\"].append(llm_output_.description_spans)\n",
    "\n",
    "        if not JUDGE_OUTPUT:\n",
    "            object_extraction_trials_no = 0\n",
    "            break\n",
    "\n",
    "        object_extraction_suggestions, _, token_count_judge = judge_objects_extractions(\n",
    "            judge_client, image, painting[\"description\"], object_and_spans, object_extraction_metrics\n",
    "        )\n",
    "        total_token_count_judge += token_count_judge\n",
    "        object_extraction_trials_no -= 1\n",
    "\n",
    "        if len(object_extraction_suggestions) != 0 and object_extraction_trials_no <= 0:\n",
    "            painting_id_to_check = painting[\"painting_id\"]\n",
    "        elif len(object_extraction_suggestions) != 0:\n",
    "            feedback = [prompt_parts, reponse_parts, object_extraction_suggestions]\n",
    "        else:\n",
    "            object_extraction_trials_no = 0\n",
    "\n",
    "    return (\n",
    "        llm_output,\n",
    "        object_and_spans,\n",
    "        painting_id_wo_objects,\n",
    "        unprocessed_painting_id,\n",
    "        painting_id_to_check,\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_object_description(\n",
    "    llm_client,\n",
    "    judge_client,\n",
    "    few_shot_examples,\n",
    "    object_and_spans,\n",
    "    object_description_metrics,\n",
    "    painting_id,\n",
    "    total_token_count,\n",
    "    total_token_count_judge,\n",
    "):\n",
    "    feedback = None\n",
    "    object_description_trials_no = 2\n",
    "    unprocessed_painting_id = None\n",
    "    painting_id_to_check = None\n",
    "\n",
    "    while object_description_trials_no > 0:\n",
    "        llm_output, token_count, prompt_parts, reponse_parts = generate(\n",
    "            llm_client,\n",
    "            few_shot_examples,\n",
    "            None,\n",
    "            None,\n",
    "            object_and_spans,\n",
    "            f\"{PROMPT_TYPE}_create_description\",\n",
    "            feedback,\n",
    "        )\n",
    "        total_token_count += token_count\n",
    "\n",
    "        if VERBOSE:\n",
    "            pprint(llm_output)\n",
    "\n",
    "        if llm_output is None:\n",
    "            unprocessed_painting_id = painting_id\n",
    "            break\n",
    "\n",
    "        if len(object_and_spans[\"object_names\"]) != len(llm_output):\n",
    "            print(\"The annotator didn't provide the number of expected descriptions.\")\n",
    "            unprocessed_painting_id = painting_id\n",
    "            break\n",
    "\n",
    "        if not JUDGE_OUTPUT:\n",
    "            object_description_trials_no = 0\n",
    "            break\n",
    "\n",
    "        object_and_spans[\"objects_description\"] = [\n",
    "            annotation.object_description for annotation in llm_output\n",
    "        ]\n",
    "        object_description_suggestions, token_count_judge, inconsistent_scoring = (\n",
    "            judge_objects_descriptions(judge_client, object_and_spans, object_description_metrics)\n",
    "        )\n",
    "        total_token_count_judge += token_count_judge\n",
    "        object_description_trials_no -= 1\n",
    "\n",
    "        if inconsistent_scoring:\n",
    "            painting_id_to_check = painting_id\n",
    "            object_description_trials_no = 0\n",
    "        if len(object_description_suggestions) != 0 and object_description_trials_no <= 0:\n",
    "            painting_id_to_check = painting_id\n",
    "        elif len(object_description_suggestions) != 0:\n",
    "            feedback = [prompt_parts, reponse_parts, object_description_suggestions]\n",
    "        else:\n",
    "            object_description_trials_no = 0\n",
    "\n",
    "    return (\n",
    "        llm_output,\n",
    "        unprocessed_painting_id,\n",
    "        painting_id_to_check,\n",
    "        total_token_count,\n",
    "        total_token_count_judge,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_mini_set(mini_set, few_shot_examples, results_name, observations):\n",
    "    # get device type\n",
    "    device = get_device()\n",
    "\n",
    "    # load models\n",
    "    llm_client = get_llm_client()\n",
    "    judge_client = get_judge_llm_client()\n",
    "    grounding_processor, grounding_model = get_grounding_model(device)\n",
    "    sentence_similarity_model = get_sentence_similarity_model()\n",
    "\n",
    "    # if an image is not included, it doesn't have annotations\n",
    "    ground_truth_bboxes, labels_to_ids = get_bbox_annotations()\n",
    "\n",
    "    # object metrics\n",
    "    micro_f1_objects = None\n",
    "    tp_fp_fn_objects = [0, 0, 0]\n",
    "    all_predicted_objects = []\n",
    "    all_ground_truth_objects = []\n",
    "\n",
    "    # spans metrics\n",
    "    micro_f1_spans = None\n",
    "    tp_fp_fn_spans = [0, 0, 0]\n",
    "    span_similarity_metrics = {\n",
    "        \"cosine similarity\": [],\n",
    "        \"Levenshtein distance\": [],\n",
    "        \"delete percentage\": [],\n",
    "        \"false positive percentage\": [],\n",
    "        \"coverage percentage\": [],\n",
    "    }\n",
    "    all_predicted_spans = []\n",
    "    all_ground_truth_spans = []\n",
    "\n",
    "    # object extraction metrics\n",
    "    object_extraction_metrics = {\n",
    "        \"objects_recall\": [],\n",
    "        \"spans_recall\": [],\n",
    "        \"objects_precision\": [],\n",
    "        \"spans_precision\": [],\n",
    "    }\n",
    "\n",
    "    # object descriptions metrics\n",
    "    object_description_metrics = {\n",
    "        \"factual_accuracy\": [],\n",
    "        \"coherence\": [],\n",
    "        \"grounding_potential\": [],\n",
    "        \"completeness\": [],\n",
    "    }\n",
    "    all_predicted_object_descriptions = []\n",
    "\n",
    "    # grounding metrics\n",
    "    map_50 = None\n",
    "    map_50_95 = None\n",
    "    all_predicted_bboxes = []\n",
    "    all_ground_truth_bboxes = []\n",
    "\n",
    "    # other tracked info\n",
    "    total_token_count = 0\n",
    "    total_token_count_judge = 0\n",
    "    painting_ids = []\n",
    "    paintings_ids_to_check = []\n",
    "    unprocessed_painting_ids = []\n",
    "    paintings_ids_wo_objects = []\n",
    "\n",
    "    for painting in tqdm(mini_set):\n",
    "        resized_image, image = load_image(painting[\"painting_id\"])\n",
    "        print(f\"PAINTING DESCRIPTION\\n{painting['description']}\")\n",
    "\n",
    "        # extract described objects\n",
    "        (\n",
    "            llm_output_objects,\n",
    "            object_and_spans,\n",
    "            painting_id_wo_objects,\n",
    "            unprocessed_painting_id,\n",
    "            painting_id_to_check,\n",
    "            total_token_count,\n",
    "            total_token_count_judge,\n",
    "        ) = extract_objects(\n",
    "            llm_client,\n",
    "            judge_client,\n",
    "            few_shot_examples,\n",
    "            resized_image,\n",
    "            object_extraction_metrics,\n",
    "            painting,\n",
    "            total_token_count,\n",
    "            total_token_count_judge,\n",
    "        )\n",
    "\n",
    "        if unprocessed_painting_id is not None:\n",
    "            unprocessed_painting_ids.append(unprocessed_painting_id)\n",
    "            print(\"UNPROCESSED PAINTING\")\n",
    "            continue\n",
    "\n",
    "        if painting_id_wo_objects is not None:\n",
    "            paintings_ids_wo_objects.append(painting_id_wo_objects)\n",
    "            print(\"PAINTING WITHOUT OBJECTS\")\n",
    "            continue\n",
    "\n",
    "        if painting_id_to_check is not None:\n",
    "            paintings_ids_to_check.append(painting_id_to_check)\n",
    "            print(\"PAINTING HAS TO BE CHECKED\")\n",
    "\n",
    "        # create description per object\n",
    "        (\n",
    "            llm_output_descriptions,\n",
    "            unprocessed_painting_id,\n",
    "            painting_id_to_check,\n",
    "            total_token_count,\n",
    "            total_token_count_judge,\n",
    "        ) = compose_object_description(\n",
    "            llm_client,\n",
    "            judge_client,\n",
    "            few_shot_examples,\n",
    "            copy.deepcopy(object_and_spans),\n",
    "            object_description_metrics,\n",
    "            painting[\"painting_id\"],\n",
    "            total_token_count,\n",
    "            total_token_count_judge,\n",
    "        )\n",
    "\n",
    "        if unprocessed_painting_id is not None:\n",
    "            unprocessed_painting_ids.append(unprocessed_painting_id)\n",
    "            print(\"UNPROCESSED PAINTING\")\n",
    "            continue\n",
    "\n",
    "        if painting_id_to_check is not None:\n",
    "            paintings_ids_to_check.append(painting_id_to_check)\n",
    "            print(\"PAINTING HAS TO BE CHECKED\")\n",
    "\n",
    "        # handle objects\n",
    "        predicted_objects, ground_truth_objects = process_objects(\n",
    "            llm_output_objects, painting, all_predicted_objects, all_ground_truth_objects\n",
    "        )\n",
    "        count_tp_fp_fn_fuzzy(predicted_objects, ground_truth_objects, tp_fp_fn_objects)\n",
    "\n",
    "        # handle spans\n",
    "        (\n",
    "            predicted_spans_per_object,\n",
    "            ground_truth_spans_per_object,\n",
    "            predicted_spans,\n",
    "            ground_truth_spans,\n",
    "        ) = process_spans(llm_output_objects, painting)\n",
    "        compute_spans_quality(\n",
    "            ground_truth_spans_per_object,\n",
    "            predicted_spans_per_object,\n",
    "            span_similarity_metrics,\n",
    "            sentence_similarity_model,\n",
    "        )\n",
    "        count_tp_fp_fn(predicted_spans, ground_truth_spans, tp_fp_fn_spans)\n",
    "        all_predicted_spans.append(predicted_spans_per_object)\n",
    "        all_ground_truth_spans.append(ground_truth_spans_per_object)\n",
    "\n",
    "        # handle object description\n",
    "        get_object_descriptions(llm_output_descriptions, all_predicted_object_descriptions)\n",
    "\n",
    "        # ground objects\n",
    "        labels_scores_boxes, grounding_results = detect_objects(\n",
    "            image,\n",
    "            predicted_objects,\n",
    "            grounding_processor,\n",
    "            grounding_model,\n",
    "            device,\n",
    "            object_threshold=0.34,\n",
    "            text_threshold=0.32,\n",
    "        )\n",
    "\n",
    "        get_bounding_boxes(\n",
    "            labels_scores_boxes,\n",
    "            labels_to_ids,\n",
    "            ground_truth_bboxes,\n",
    "            painting[\"painting_id\"],\n",
    "            all_predicted_bboxes,\n",
    "            all_ground_truth_bboxes,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # if this line has been reached, the full image processing is done\n",
    "        painting_ids.append(painting[\"painting_id\"])\n",
    "\n",
    "    # compute metrics across the entire dataset\n",
    "    micro_f1_objects = compute_micro_f1(tp_fp_fn_objects, \"objects\")\n",
    "\n",
    "    micro_f1_spans = compute_micro_f1(tp_fp_fn_spans, \"spans\")\n",
    "    for metric in span_similarity_metrics:\n",
    "        span_similarity_metrics[metric] = np.array(span_similarity_metrics[metric]).mean()\n",
    "\n",
    "    for metric in object_description_metrics:\n",
    "        object_description_metrics[metric] = np.array(object_description_metrics[metric]).mean()\n",
    "\n",
    "    for metric in object_extraction_metrics:\n",
    "        object_extraction_metrics[metric] = np.array(object_extraction_metrics[metric]).mean()\n",
    "\n",
    "    map_50, map_50_95 = compute_mean_average_precision(\n",
    "        all_predicted_bboxes, all_ground_truth_bboxes, device\n",
    "    )\n",
    "\n",
    "    # store results for the tested prompt\n",
    "    predictions = []\n",
    "\n",
    "    assert len(all_predicted_spans) == len(all_predicted_object_descriptions)\n",
    "\n",
    "    for index, painting_data in enumerate(all_predicted_spans):\n",
    "        object_descriptions = all_predicted_object_descriptions[index]\n",
    "        object_data = {}\n",
    "\n",
    "        for object_index, (object, spans) in enumerate(painting_data.items()):\n",
    "            object_data[object] = [spans, object_descriptions[object_index]]\n",
    "\n",
    "        predictions.append(object_data)\n",
    "\n",
    "    results_values = list(zip(painting_ids, predictions, all_ground_truth_spans))\n",
    "\n",
    "    metrics = {\n",
    "        \"total_token_count_annotator\": total_token_count,\n",
    "        \"total_token_count_judge\": total_token_count_judge,\n",
    "        \"unprocessed_painting_ids\": unprocessed_painting_ids,\n",
    "        \"paintings_ids_to_check\": paintings_ids_to_check,\n",
    "        \"paintings_ids_wo_objects\": paintings_ids_wo_objects,\n",
    "        \"micro_f1_objects\": micro_f1_objects,\n",
    "        \"micro_f1_spans\": micro_f1_spans,\n",
    "        \"span_similarity_metrics\": span_similarity_metrics,\n",
    "        \"object_description_metrics\": object_description_metrics,\n",
    "        \"object_extraction_metrics\": object_extraction_metrics,\n",
    "        \"map_50\": map_50,\n",
    "        \"map_50_95\": map_50_95,\n",
    "    }\n",
    "\n",
    "    store_results(PROMPT_TYPE, results_name, observations, results_values, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_observations = \"enhanced_3\"\n",
    "paintings_data, annotations, few_shot_examples, mini_val_set, mini_test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_mini_set(mini_val_set, few_shot_examples, \"mini_val_set\", experiment_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_mini_set(mini_test_set, few_shot_examples, \"mini_test_set\", experiment_observations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
