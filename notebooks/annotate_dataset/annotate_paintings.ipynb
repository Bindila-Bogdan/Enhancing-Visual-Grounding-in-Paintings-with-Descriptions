{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate paintings\n",
    "This notebook leverages an LLM to extract the described objects that appear in a painting and then ground them with Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from nltk.corpus import stopwords\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "GROUNDING_MODEL_ID = \"IDEA-Research/grounding-dino-base\"\n",
    "\n",
    "\n",
    "RAW_DATA_PATH = \"../../data/raw/\"\n",
    "ANNOTATIONS_PATH = \"../../data/annotations/\"\n",
    "RESULTS_PATH = \"../../experiments/prompting/\"\n",
    "INTERMEDIATE_DATA_PATH = \"../../data/intermediate/filtered_paintings/\"\n",
    "\n",
    "FEW_SHOT_EXAMPLES_IDS = [2156, 2484, 11819, 256, 10748, 3344, 10676]\n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../config/keys.json\", \"r\") as file:\n",
    "    os.environ[\"GEMINI_API_KEY\"] = json.load(file)[\"gemini_api_key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract with an LLM the described objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_available_models(client):\n",
    "    existing_models = [\n",
    "        model.name.split(\"/\")[1]\n",
    "        for model in client.models.list()\n",
    "        if \"generateContent\" in model.supported_actions\n",
    "    ]\n",
    "\n",
    "    print(existing_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_bytes(image):\n",
    "    # define an in-memory byte stream\n",
    "    img_byte_array = io.BytesIO()\n",
    "\n",
    "    # convert the image to a byte representation and store it in the in-memory byte stream\n",
    "    image.save(img_byte_array, format=\"PNG\")\n",
    "\n",
    "    # get the byte representation of the image\n",
    "    img_bytes = img_byte_array.getvalue()\n",
    "\n",
    "    return img_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Define prompts to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_prompt(examples, image, description):\n",
    "    prompt_parts = []\n",
    "    prompt_parts.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=\"\\nHere are some examples:\")]))\n",
    "\n",
    "    for example in examples:\n",
    "        example_painting_id = example[\"painting_id\"]\n",
    "        example_description = example[\"description\"]\n",
    "        example_detected_objects = \", \".join(example[\"object_name\"])\n",
    "        example_image = Image.open(f\"{RAW_DATA_PATH}filtered_paintings/{example_painting_id}.png\")\n",
    "\n",
    "        prompt_parts.append(types.Content(role=\"user\", parts=[\n",
    "            types.Part.from_bytes(mime_type=\"image/png\", data=image_to_bytes(example_image)),\n",
    "            types.Part.from_text(text=f\"Description: \\\"\\\"\\\"{example_description}\\\"\\\"\\\"\")\n",
    "        ]))\n",
    "        prompt_parts.append(types.Content(role=\"model\", parts=[\n",
    "            types.Part.from_text(text=f\"Detected objects: {example_detected_objects}\")\n",
    "        ]))\n",
    "        prompt_parts.append(types.Content(role=\"user\", parts=[types.Part.from_text(text=\"---\")]))\n",
    "\n",
    "    prompt_parts.append(types.Content(role=\"user\", parts=[\n",
    "        types.Part.from_bytes(mime_type=\"image/png\", data=image_to_bytes(image)),\n",
    "        types.Part.from_text(text=f\"Description: \\\"\\\"\\\"{description}\\\"\\\"\\\"\\n\\nList separated by comma of **ONLY** the objects (lowercased) described in the given painting that also appear in the textual description.\")\n",
    "    ]))\n",
    "\n",
    "\n",
    "    system_prompt_text = \"\"\"You are an expert in art who can identify objects present in both a painting and its textual description.\"\"\"\n",
    "\n",
    "    return prompt_parts, system_prompt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(examples, image, description):\n",
    "    if PROMPT_TYPE == \"basic\":\n",
    "        return get_basic_prompt(examples, image, description)\n",
    "    else:\n",
    "        raise \"Unknown prompt type\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Call the LLM to annotate the painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(client, examples, image, description):\n",
    "    prompt_parts, system_prompt_text = get_prompt(examples, image, description)\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        response_mime_type=\"text/plain\",\n",
    "        system_instruction=[\n",
    "            types.Part.from_text(text=system_prompt_text),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=prompt_parts,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "\n",
    "    output = response.text\n",
    "    prompt_tokens_count = response.usage_metadata.prompt_token_count\n",
    "    output_tokens_count = response.usage_metadata.candidates_token_count\n",
    "    total_token_count = prompt_tokens_count + output_tokens_count\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"Prompt tokens count: {prompt_tokens_count}\\nOutput tokens count: {output_tokens_count}\")\n",
    "        print(f\"Response:\\n{output}\\n\")\n",
    "\n",
    "    return output, total_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ground Objects with Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_annotated_image(image, labels_scores_boxes):\n",
    "    font = ImageFont.truetype(\"../../config/alata-regular.ttf\", 18)\n",
    "    draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "    for label, score, coords in labels_scores_boxes:\n",
    "        random_color = \"#{:06x}\".format(random.randint(0, 0xFFFFFF)) + \"80\"\n",
    "        text_position = (coords[0] + 10, coords[1] + 5)\n",
    "\n",
    "        draw.rectangle(coords, outline=random_color, width=5)\n",
    "        draw.text(text_position, label + \" \" + str(round(score, 2)), fill=random_color, font=font)\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image, text, processor, model, object_threshold=0.3, text_threshold=0.3):\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        # threshold for filtering object detection predictions (lower -> more bounding boxes)\n",
    "        threshold=object_threshold,\n",
    "        # threshold for filtering text detection predictions (lower -> the input text is taken exactly)\n",
    "        text_threshold=text_threshold,\n",
    "        target_sizes=[image.size[::-1]],\n",
    "    )\n",
    "\n",
    "    assert len(results) == 1\n",
    "\n",
    "    labels = results[0][\"text_labels\"]\n",
    "    scores = results[0][\"scores\"].cpu().numpy()\n",
    "    box_coordinates = [list(coords) for coords in results[0][\"boxes\"].cpu().numpy()]\n",
    "    labels_scores_boxes = sorted(list(zip(labels, scores, box_coordinates)), key=lambda x: x[1])\n",
    "\n",
    "    for label, score, coords in labels_scores_boxes:\n",
    "        print(label, float(score), [float(coord) for coord in coords])\n",
    "\n",
    "    display_annotated_image(image, labels_scores_boxes)\n",
    "\n",
    "    return labels_scores_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Annotate painting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    paintings_data = pl.read_json(f\"{INTERMEDIATE_DATA_PATH}filtered_paintings_enhanced_data.json\")\n",
    "    annotations = pl.read_json(ANNOTATIONS_PATH + \"manual_annotations.json\").with_columns(pl.col(\"object_name\").str.replace_all(',','',literal=True).alias(\"object_name\"))\n",
    "   \n",
    "    few_shots_descriptions = paintings_data.filter(pl.col(\"id\").is_in(FEW_SHOT_EXAMPLES_IDS)).select(\"id\", \"description\").rename({\"id\": \"painting_id\"})\n",
    "    few_shot_examples = ((\n",
    "        annotations.filter(pl.col(\"painting_id\").is_in(FEW_SHOT_EXAMPLES_IDS))\n",
    "        .group_by(\"painting_id\")\n",
    "        .agg(pl.col(\"*\"))\n",
    "        .select(\"painting_id\", \"object_name\", \"description_spans\")\n",
    "    ).join(few_shots_descriptions, on=\"painting_id\")).to_dicts()\n",
    "\n",
    "    test_descriptions = paintings_data.filter(~pl.col(\"id\").is_in(FEW_SHOT_EXAMPLES_IDS)).select(\"id\", \"description\").rename({\"id\": \"painting_id\"})\n",
    "    test_paintings = ((annotations.filter(~pl.col(\"painting_id\").is_in(FEW_SHOT_EXAMPLES_IDS))\n",
    "        .group_by(\"painting_id\")\n",
    "        .agg(pl.col(\"*\"))\n",
    "        .select(\"painting_id\", \"object_name\", \"description_spans\")\n",
    "    ).join(test_descriptions, on=\"painting_id\")).to_dicts()\n",
    "\n",
    "    return paintings_data, annotations, few_shot_examples, test_paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_object_name(object_name):\n",
    "    input_words = object_name.lower().split(\" \")\n",
    "    cleaned_object_name = \" \".join([word.replace(\"\\n\", \"\") for word in input_words if word.replace(\"\\n\", \"\") not in STOP_WORDS])\n",
    "\n",
    "    return cleaned_object_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(predictions, ground_truth, tp_fp_fn):\n",
    "    for prediction in predictions:\n",
    "        if prediction in ground_truth:\n",
    "            tp_fp_fn[0] += 1\n",
    "            ground_truth.remove(prediction)\n",
    "        else:\n",
    "            tp_fp_fn[1] += 1\n",
    "\n",
    "    tp_fp_fn[2] = len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_micro_f1(tp_fp_fn):\n",
    "    if tp_fp_fn[0] + tp_fp_fn[1] == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp_fp_fn[0] / (tp_fp_fn[0] + tp_fp_fn[1])\n",
    "\n",
    "    if tp_fp_fn[0] + tp_fp_fn[2] == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp_fp_fn[0] / (tp_fp_fn[0] + tp_fp_fn[2])\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return round(f1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(micro_f1, results_values):\n",
    "    results_file_name = f\"{RESULTS_PATH}prompting_results.json\"\n",
    "\n",
    "    try:\n",
    "        with open(results_file_name, \"r\") as file:\n",
    "            all_results = json.load(file)\n",
    "    except:\n",
    "            all_results = None\n",
    "\n",
    "    results = {\"prompt_type\": PROMPT_TYPE,\n",
    "    \"observations\": OBSERVATIONS,\n",
    "    \"micro_f1\": micro_f1,\n",
    "    \"results\": results_values}\n",
    "\n",
    "    if all_results is None:\n",
    "        all_results = [results]\n",
    "    else:\n",
    "        all_results.append(results)\n",
    "\n",
    "    with open(results_file_name, \"w\") as file:\n",
    "        json.dump(all_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Experiment with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "PROMPT_TYPE = \"basic\"\n",
    "OBSERVATIONS = \"first trial with few-shot learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LLM client\n",
    "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# load Grounding DINO\n",
    "processor = AutoProcessor.from_pretrained(GROUNDING_MODEL_ID)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(GROUNDING_MODEL_ID).to(DEVICE)\n",
    "\n",
    "# load data\n",
    "paintings_data, annotations, few_shot_examples, test_paintings = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_fp_fn = [0, 0, 0]\n",
    "total_token_count = 0\n",
    "\n",
    "painting_ids = []\n",
    "all_predicted_objects = []\n",
    "all_ground_truth_objects = []\n",
    "\n",
    "for painting in tqdm(test_paintings[:2]):\n",
    "    painting_id = painting[\"painting_id\"]\n",
    "    painting_ids.append(painting_id)\n",
    "    ground_truth_objects = copy.deepcopy(painting[\"object_name\"])\n",
    "    description = copy.deepcopy(painting[\"description\"])\n",
    "    image = Image.open(f\"../../data/raw/filtered_paintings/{painting_id}.png\")\n",
    "\n",
    "    # extract described objects\n",
    "    output, token_count = generate(client, few_shot_examples, image, description)\n",
    "    total_token_count += token_count\n",
    "\n",
    "    # compute metrics\n",
    "    predicted_objects = sorted([clean_object_name(object_name) for object_name in output.split(\", \")])\n",
    "    all_predicted_objects.append(predicted_objects)\n",
    "    ground_truth_objects = sorted([clean_object_name(object_name) for object_name in ground_truth_objects])\n",
    "    all_ground_truth_objects.append(ground_truth_objects)\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(predicted_objects, ground_truth_objects)\n",
    "        \n",
    "    compute_f1(copy.deepcopy(predicted_objects), copy.deepcopy(ground_truth_objects), tp_fp_fn)\n",
    "\n",
    "    time.sleep(5)\n",
    "    \n",
    "micro_f1 = compute_micro_f1(tp_fp_fn)\n",
    "print(f\"Micro F1: {micro_f1}\")\n",
    "print(f\"Total token count: {total_token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results for the tested prompt\n",
    "results_values = list(zip(painting_ids, all_predicted_objects, all_ground_truth_objects))\n",
    "store_results(micro_f1, results_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground objects\n",
    "text = \", \".join(predicted_objects) + \".\"\n",
    "labels_scores_boxes = detect_objects(image, text, processor, model, object_threshold=0.3, text_threshold=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
