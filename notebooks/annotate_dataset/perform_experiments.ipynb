{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd401b6",
   "metadata": {},
   "source": [
    "# Perform experiments\n",
    "This notebook is used to performed different experiments such as finding the optimal hyperparameters for Grounding DINO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04716b3f",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "from ground_objects import *\n",
    "from compute_metrics import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_PATH = \"../../data/annotations/\"\n",
    "GROUNDING_RESULTS_PATH = \"../../experiments/grounding/\"\n",
    "\n",
    "INTERMEDIATE_DATA_PATH = \"../../data/intermediate/filtered_paintings/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fcdd7e",
   "metadata": {},
   "source": [
    "### 1. Find the optimal hyperparameters for Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d755d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_val_set_ids = [\n",
    "    91,\n",
    "    256,\n",
    "    517,\n",
    "    518,\n",
    "    549,\n",
    "    723,\n",
    "    1006,\n",
    "    1056,\n",
    "    1067,\n",
    "    1667,\n",
    "    1722,\n",
    "    1753,\n",
    "    1799,\n",
    "    1823,\n",
    "    1966,\n",
    "    2024,\n",
    "    2156,\n",
    "    2225,\n",
    "    2441,\n",
    "    2484,\n",
    "    2628,\n",
    "    2737,\n",
    "    3344,\n",
    "    3687,\n",
    "    4416,\n",
    "    5551,\n",
    "    6660,\n",
    "    8316,\n",
    "    8344,\n",
    "    10748,\n",
    "    11422,\n",
    "    11819,\n",
    "]\n",
    "mini_test_set_ids = [1466, 2461, 2549, 2613, 2657, 2702, 8197, 9053, 9821, 11593]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c83bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pl.read_json(ANNOTATIONS_PATH + \"manual_annotations.json\").with_columns(\n",
    "    pl.col(\"object_name\").str.replace_all(\",\", \"\", literal=True).alias(\"object_name\")\n",
    ")\n",
    "mini_val_set = (\n",
    "    annotations.filter(pl.col(\"painting_id\").is_in(mini_val_set_ids))\n",
    "    .group_by(\"painting_id\")\n",
    "    .agg(pl.col(\"object_name\"))\n",
    "    .to_numpy()\n",
    ")\n",
    "mini_test_set = (\n",
    "    annotations.filter(pl.col(\"painting_id\").is_in(mini_test_set_ids))\n",
    "    .group_by(\"painting_id\")\n",
    "    .agg(pl.col(\"object_name\"))\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device type\n",
    "device = get_device()\n",
    "\n",
    "# load groudning model\n",
    "grounding_processor, grounding_model = get_grounding_model(device)\n",
    "\n",
    "# if an image is not included, it doesn't have annotations\n",
    "ground_truth_bboxes, labels_to_ids = get_bbox_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e394a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_all_objects(\n",
    "    dataset,\n",
    "    ground_truth_bboxes,\n",
    "    labels_to_ids,\n",
    "    grounding_processor,\n",
    "    grounding_model,\n",
    "    device,\n",
    "    object_threshold=0.3,\n",
    "    text_threshold=0.3,\n",
    "):\n",
    "    mape = 0\n",
    "    paintings_no = 0\n",
    "    all_predicted_bboxes = []\n",
    "    all_ground_truth_bboxes = []\n",
    "\n",
    "    for painting_id, objects in dataset:\n",
    "        _, image = load_image(painting_id)\n",
    "\n",
    "        labels_scores_boxes, _ = detect_objects(\n",
    "            image,\n",
    "            objects,\n",
    "            grounding_processor,\n",
    "            grounding_model,\n",
    "            device,\n",
    "            object_threshold=object_threshold,\n",
    "            text_threshold=text_threshold,\n",
    "        )\n",
    "\n",
    "        for pred in labels_scores_boxes:\n",
    "            if pred[0] == \"\":\n",
    "                return None, None\n",
    "\n",
    "        get_bounding_boxes(\n",
    "            labels_scores_boxes,\n",
    "            labels_to_ids,\n",
    "            ground_truth_bboxes,\n",
    "            painting_id,\n",
    "            all_predicted_bboxes,\n",
    "            all_ground_truth_bboxes,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        gt_bboxes_no = len(\n",
    "            [gt_bboxes for gt_bboxes in ground_truth_bboxes if gt_bboxes[\"image_id\"] == painting_id]\n",
    "        )\n",
    "        pred_bboxes_no = len(labels_scores_boxes)\n",
    "\n",
    "        if gt_bboxes_no != 0:\n",
    "            mape += abs(gt_bboxes_no - pred_bboxes_no) / gt_bboxes_no\n",
    "            paintings_no += 1\n",
    "\n",
    "    map_50, _ = compute_mean_average_precision(\n",
    "        all_predicted_bboxes, all_ground_truth_bboxes, device\n",
    "    )\n",
    "\n",
    "    mape /= paintings_no\n",
    "\n",
    "    return map_50, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383961b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_thresholds = np.arange(0.3, 0.351, 0.01)\n",
    "text_thresholds = np.arange(0.3, 0.351, 0.01)\n",
    "grounding_results = {\n",
    "    \"object_threshold\": [],\n",
    "    \"text_threshold\": [],\n",
    "    \"map_50_val\": [],\n",
    "    \"mape_val\": [],\n",
    "    \"map_50_test\": [],\n",
    "    \"mape_test\": [],\n",
    "}\n",
    "\n",
    "for object_threshold, text_threshold in tqdm(list(product(object_thresholds, text_thresholds))):\n",
    "    map_50_val, mape_val = ground_all_objects(\n",
    "        mini_val_set,\n",
    "        ground_truth_bboxes,\n",
    "        labels_to_ids,\n",
    "        grounding_processor,\n",
    "        grounding_model,\n",
    "        device,\n",
    "        object_threshold,\n",
    "        text_threshold,\n",
    "    )\n",
    "\n",
    "    if map_50_val is None:\n",
    "        continue\n",
    "\n",
    "    map_50_test, mape_test = ground_all_objects(\n",
    "        mini_test_set,\n",
    "        ground_truth_bboxes,\n",
    "        labels_to_ids,\n",
    "        grounding_processor,\n",
    "        grounding_model,\n",
    "        device,\n",
    "        object_threshold,\n",
    "        text_threshold,\n",
    "    )\n",
    "\n",
    "    if map_50_test is None:\n",
    "        continue\n",
    "\n",
    "    grounding_results[\"object_threshold\"].append(object_threshold)\n",
    "    grounding_results[\"text_threshold\"].append(text_threshold)\n",
    "    grounding_results[\"map_50_val\"].append(map_50_val)\n",
    "    grounding_results[\"mape_val\"].append(mape_val)\n",
    "    grounding_results[\"map_50_test\"].append(map_50_test)\n",
    "    grounding_results[\"mape_test\"].append(mape_test)\n",
    "\n",
    "    grounding_results_df = pl.from_dict(grounding_results)\n",
    "    # grounding_results_df.write_csv(f\"{GROUNDING_RESULTS_PATH}grounding_dino_hyperparameters.csv\")\n",
    "\n",
    "grounding_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "grounding_results_df = pl.read_csv(f\"{GROUNDING_RESULTS_PATH}grounding_dino_hyperparameters.csv\")\n",
    "grounding_results_df = (\n",
    "    grounding_results_df.with_columns((1 - pl.col(\"mape_val\")).alias(\"1-mape_val\"))\n",
    "    .with_columns(((pl.col(\"map_50_val\") + pl.col(\"1-mape_val\")) / 2).alias(\"avg metric val\"))\n",
    "    .with_columns((1 - pl.col(\"mape_test\")).alias(\"1-mape_test\"))\n",
    "    .with_columns(((pl.col(\"map_50_test\") + pl.col(\"1-mape_test\")) / 2).alias(\"avg metric test\"))\n",
    "    .drop(\"mape_val\", \"mape_test\")\n",
    ")\n",
    "\n",
    "\n",
    "grounding_results_df.sort(\"avg metric val\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_50_test, diff_obj_no = ground_all_objects(\n",
    "    mini_val_set,\n",
    "    ground_truth_bboxes,\n",
    "    labels_to_ids,\n",
    "    grounding_processor,\n",
    "    grounding_model,\n",
    "    device,\n",
    "    0.34,\n",
    "    0.32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ac798",
   "metadata": {},
   "source": [
    "### 2. Analyze what to remove from descriptions to clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8330654",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_painting_ids = pl.read_json(ANNOTATIONS_PATH + \"manual_annotations.json\")[\n",
    "    \"painting_id\"\n",
    "].to_list()\n",
    "descriptions = (\n",
    "    pl.read_json(f\"{INTERMEDIATE_DATA_PATH}filtered_paintings_enhanced_data.json\")\n",
    "    .filter(pl.col(\"id\").is_in(annotated_painting_ids))[\"source\", \"description\"]\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_in_chunks(text, chunk_size=24):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        print(' '.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_description(text):\n",
    "    # remove [url href=...]...[/url], keep inner text\n",
    "    text = re.sub(r'\\[url href=.*?\\](.*?)\\[/url\\]', r'\\1', text)\n",
    "    \n",
    "    # remove [i], [/i], [b], [/b], [u], [/u]\n",
    "    text = re.sub(r'\\[/?[ibu]\\]', '', text)\n",
    "    \n",
    "    # remove raw URLs\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # remove remaining url tags\n",
    "    text = re.sub(r'\\[/url\\]', '', text)\n",
    "    text = re.sub(r'\\[url=?', '', text)\n",
    "\n",
    "    # collapse multiple spaces and strip whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c54daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, description in descriptions:\n",
    "    original_description = description\n",
    "    cleaned_description = clean_description(description)\n",
    "\n",
    "    print(f\"---{source}--- {original_description == cleaned_description}\")\n",
    "\n",
    "    if original_description != cleaned_description:\n",
    "        print_in_chunks(original_description)\n",
    "        print(\"+++\")\n",
    "        print_in_chunks(cleaned_description)\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
