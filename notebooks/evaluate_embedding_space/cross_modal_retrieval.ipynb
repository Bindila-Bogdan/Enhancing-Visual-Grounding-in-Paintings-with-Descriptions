{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08f95b2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Cross modal retrieval\n",
    "This notebook is used to evaluate the quality of the embedding space through cross-modal retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3927a7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 0. Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "DETAILED_EMBEDDING_SPACE_EVALUATION = True\n",
    "\n",
    "if DETAILED_EMBEDDING_SPACE_EVALUATION:\n",
    "    INPUT_PATH_EMBEDDINGS = \"../../data/embeddings/\"\n",
    "    EMBEDDINGS_FILE_NAME = (\n",
    "        \"baseline_embeddings_test_projections_text_embedding_enhanced_features.json\"\n",
    "    )\n",
    "    IMAGES_PATH = \"../../data/fine-tuning_clip/\"\n",
    "\n",
    "else:\n",
    "    INPUT_PATH_EMBEDDINGS = \"../../../Open-Grounding-DINO/embeddings_data/\"\n",
    "    EMBEDDINGS_FILE_NAME = \"clip_embeddings_test_clip_full_2e_6_diff_lr.json\"\n",
    "    IMAGES_PATH = (\n",
    "        \"../../../Enhancing-Visual-Grounding-in-Paintings-with-Descriptions/data/fine-tuning_clip/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embeddings_data = pl.read_json(\n",
    "        f\"{INPUT_PATH_EMBEDDINGS}{EMBEDDINGS_FILE_NAME}\", infer_schema_length=1000\n",
    "    ).explode(pl.all())\n",
    "except:\n",
    "    embeddings_data = pl.read_json(\n",
    "        f\"{INPUT_PATH_EMBEDDINGS}{EMBEDDINGS_FILE_NAME}\", infer_schema_length=1000\n",
    "    )\n",
    "\n",
    "try:\n",
    "    embeddings_data = embeddings_data.with_row_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "indices = embeddings_data[\"index\"].to_list()\n",
    "embeddings_data = embeddings_data.with_columns(\n",
    "    (f\"test/\" + pl.col(\"index\").cast(pl.String) + \".png\").alias(\"image_name\")\n",
    ").rename({\"object_description\": \"text\"})\n",
    "embeddings_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f2bb1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 1. Define functions to perform retrieval and measure metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac53d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(\n",
    "    embeddings_data, description_embeddings, image_object_embeddings, query_index, query_type\n",
    "):\n",
    "\n",
    "    if query_type == \"description\":\n",
    "        # get object image embeddings based on description embedding\n",
    "        description_query = embeddings_data[\"text\"][query_index]\n",
    "        relevant_document_ids = embeddings_data.filter(pl.col(\"text\") == description_query)[\n",
    "            \"index\"\n",
    "        ].to_numpy()\n",
    "\n",
    "        similarities = cosine_similarity(\n",
    "            np.array(description_embeddings[query_index]).reshape(1, -1),\n",
    "            np.array(image_object_embeddings),\n",
    "        )[0]\n",
    "\n",
    "        ranked_document_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_similarities = np.sort(similarities)[::-1]\n",
    "\n",
    "    elif query_type == \"image\":\n",
    "        # get description embedding based on image object embedding\n",
    "        corresponding_descriptions = embeddings_data[\"text\"][query_index]\n",
    "        relevant_document_ids = embeddings_data.filter(\n",
    "            pl.col(\"text\") == corresponding_descriptions\n",
    "        )[\"index\"].to_numpy()\n",
    "\n",
    "        similarities = cosine_similarity(\n",
    "            np.array(image_object_embeddings[query_index]).reshape(1, -1),\n",
    "            np.array(description_embeddings),\n",
    "        )[0]\n",
    "\n",
    "        ranked_document_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_similarities = np.sort(similarities)[::-1]\n",
    "\n",
    "    else:\n",
    "        raise NameError(\"This query type does not exist.\")\n",
    "    \n",
    "    return relevant_document_ids, ranked_document_indices, sorted_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e2cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    embeddings_data, description_embeddings, image_object_embeddings, indices, query_type\n",
    "):\n",
    "    hit_at_1 = []\n",
    "    hit_at_5 = []\n",
    "    hit_at_10 = []\n",
    "    first_rank = []\n",
    "    reciprocal_first_rank = []\n",
    "\n",
    "    for query_index in tqdm(indices):\n",
    "\n",
    "        relevant_document_ids, ranked_document_indices, sorted_similarities = retrieve_documents(\n",
    "            embeddings_data,\n",
    "            description_embeddings,\n",
    "            image_object_embeddings,\n",
    "            query_index,\n",
    "            query_type,\n",
    "        )\n",
    "\n",
    "        hit_at_1.append(int(np.isin(relevant_document_ids, ranked_document_indices[:1]).any()))\n",
    "        hit_at_5.append(int(np.isin(relevant_document_ids, ranked_document_indices[:5]).any()))\n",
    "        hit_at_10.append(int(np.isin(relevant_document_ids, ranked_document_indices[:10]).any()))\n",
    "        first_rank.append(\n",
    "            np.where(np.isin(ranked_document_indices, relevant_document_ids) == True)[0][0] + 1\n",
    "        )\n",
    "        reciprocal_first_rank.append(1 / first_rank[-1])\n",
    "\n",
    "    hit_rate_at_1 = np.array(hit_at_1).mean()\n",
    "    hit_rate_at_5 = np.array(hit_at_5).mean()\n",
    "    hit_rate_at_10 = np.array(hit_at_10).mean()\n",
    "    median_rank = round(np.median(np.array(first_rank)), 2)\n",
    "    mean_reciprocal_rank = round(np.array(reciprocal_first_rank).mean(), 4)\n",
    "\n",
    "    print(\n",
    "        f\"Hit@1: {hit_rate_at_1}\\nHit@5: {hit_rate_at_5}\\nHit@10: {hit_rate_at_10}\\nMedian Rank: {median_rank}\\nMRR: {mean_reciprocal_rank}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8bdc5f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 2. Perform retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0d3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_object_embeddings = embeddings_data[\"embedding_object_image\"].to_list()\n",
    "\n",
    "try:\n",
    "    description_embeddings_enhanced = embeddings_data[\"text_embedding_enhanced\"].to_list()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    description_embeddings_backbone = embeddings_data[\"text_embedding_backbone\"].to_list()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90186f9b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 2.1. Use image objects as queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da1001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    evaluate_retrieval(\n",
    "        embeddings_data, description_embeddings_backbone, image_object_embeddings, indices, \"image\"\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bec1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    evaluate_retrieval(\n",
    "        embeddings_data, description_embeddings_enhanced, image_object_embeddings, indices, \"image\"\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c4d52",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 2.2. Use descriptions as queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d4189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    evaluate_retrieval(\n",
    "        embeddings_data,\n",
    "        description_embeddings_backbone,\n",
    "        image_object_embeddings,\n",
    "        indices,\n",
    "        \"description\",\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352f172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    evaluate_retrieval(\n",
    "        embeddings_data,\n",
    "        description_embeddings_enhanced,\n",
    "        image_object_embeddings,\n",
    "        indices,\n",
    "        \"description\",\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537a897",
   "metadata": {},
   "source": [
    "### 3. Perform retrieval based on type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ebb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_data[\"coarse_type\"].value_counts().sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_types = set(\n",
    "    embeddings_data.filter(pl.col(\"coarse_type\").is_not_null())[\"coarse_type\"].to_list()\n",
    ")\n",
    "\n",
    "coarse_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c79158",
   "metadata": {},
   "source": [
    "#### 3.1. Use image objects as queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66dfb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coarse_type in coarse_types:\n",
    "    print(f\"{\"-\" * len(coarse_type)}\\n{coarse_type}\")\n",
    "    indices_current_type = embeddings_data.filter(pl.col(\"coarse_type\") == coarse_type)[\n",
    "        \"index\"\n",
    "    ].to_list()\n",
    "    evaluate_retrieval(\n",
    "        embeddings_data,\n",
    "        description_embeddings_enhanced,\n",
    "        image_object_embeddings,\n",
    "        indices_current_type,\n",
    "        \"image\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8699a01",
   "metadata": {},
   "source": [
    "#### 3.2. Use descriptions as queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c36c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coarse_type in coarse_types:\n",
    "    print(f\"{\"-\" * len(coarse_type)}\\n{coarse_type}\")\n",
    "    indices_current_type = embeddings_data.filter(pl.col(\"coarse_type\") == coarse_type)[\n",
    "        \"index\"\n",
    "    ].to_list()\n",
    "    evaluate_retrieval(\n",
    "        embeddings_data,\n",
    "        description_embeddings_enhanced,\n",
    "        image_object_embeddings,\n",
    "        indices_current_type,\n",
    "        \"description\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8314111-97bb-4336-83d7-22721a9d98be",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 4. Perform visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_retrieval(images, descriptions, labels, output_filename=\"\", font_size=8): # font_size=12\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(12, 5)) # (18, 8)\n",
    "\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.1, hspace=0.6, wspace=0.1)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        ax.imshow(images[i])\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Query [{labels[i]}]\", fontsize=font_size + 4)\n",
    "        else:\n",
    "            ax.set_title(f\"Rank {i} [{labels[i]}]\", fontsize=font_size + 4)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        wrapped_text = textwrap.fill(descriptions[i], width=30)\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.05,\n",
    "            wrapped_text,\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=font_size,\n",
    "            wrap=True,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if output_filename != \"\":\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f967655",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_embeddings = (\n",
    "    pl.read_json(\n",
    "        f\"{INPUT_PATH_EMBEDDINGS}baseline_embeddings_test_projections_text_embedding_enhanced_features.json\",\n",
    "        infer_schema_length=1000,\n",
    "    ).sort(\"painting_id\")\n",
    "    .with_row_index()\n",
    "    .with_columns((f\"test/\" + pl.col(\"index\").cast(pl.String) + \".png\").alias(\"image_name\"))\n",
    "    .with_columns((pl.col(\"year\") // 100 + 1).alias(\"century\"))\n",
    "    .rename({\"object_description\": \"text\"})\n",
    ")\n",
    "probabilities = projected_embeddings[\"probability\"]\n",
    "projected_embeddings = projected_embeddings.sort(\"probability\", descending=True).unique(subset=[\"text\"], keep=\"first\").sort(\"index\").drop(\"index\").with_row_index()\n",
    "\n",
    "description_projected_embeddings = projected_embeddings[\"text_embedding_enhanced\"].to_list()\n",
    "image_object_projected_embeddings = projected_embeddings[\"embedding_object_image\"].to_list()\n",
    "projected_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embeddings = (\n",
    "    pl.read_json(\n",
    "        f\"{INPUT_PATH_EMBEDDINGS}clip_embeddings_test_clip_full_1e_6_diff_lr_not_frozen_features.json\",\n",
    "        infer_schema_length=1000,\n",
    "    )\n",
    "    .with_columns((f\"test/\" + pl.col(\"index\").cast(pl.String) + \".png\").alias(\"image_name\"))\n",
    "    .with_columns((pl.col(\"year\") // 100 + 1).alias(\"century\"))\n",
    "    .rename({\"object_description\": \"text\"})\n",
    ").with_columns(pl.Series(probabilities).alias(\"probability\"))\n",
    "clip_embeddings = clip_embeddings.sort(\"probability\", descending=True).unique(subset=[\"text\"], keep=\"first\").sort(\"index\").drop(\"index\").with_row_index()\n",
    "\n",
    "description_clip_embeddings = clip_embeddings[\"text_embedding_enhanced\"].to_list()\n",
    "image_object_clip_embeddings = clip_embeddings[\"embedding_object_image\"].to_list()\n",
    "clip_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8976cc-c2bd-4cf8-a94e-ae625487c46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_retrieval_results(embeddings_data, description_embeddings, image_object_embeddings, query_index, query_modality, top_k=5):\n",
    "    _, ranked_document_indices, _ = retrieve_documents(\n",
    "        embeddings_data,\n",
    "        description_embeddings,\n",
    "        image_object_embeddings,\n",
    "        query_index=query_index,\n",
    "        query_type=query_modality,\n",
    "    )\n",
    "\n",
    "    query_image = [Image.open(f\"{IMAGES_PATH}{embeddings_data['image_name'][int(query_index)]}\")]\n",
    "    query_description = [embeddings_data[\"text\"][int(query_index)]]\n",
    "    query_label = [embeddings_data[\"label\"][int(query_index)]]\n",
    "\n",
    "    retrieved_images = [\n",
    "        Image.open(f\"{IMAGES_PATH}{embeddings_data['image_name'][int(index)]}\")\n",
    "        for index in ranked_document_indices[:top_k]\n",
    "    ]\n",
    "\n",
    "    retrieved_descriptions = [\n",
    "        embeddings_data[\"text\"][int(index)] for index in ranked_document_indices[:top_k]\n",
    "    ]\n",
    "\n",
    "    retrieved_labels = [\n",
    "        embeddings_data[\"label\"][int(index)] for index in ranked_document_indices[:top_k]\n",
    "    ]\n",
    "\n",
    "    visualize_retrieval(\n",
    "        query_image + retrieved_images, query_description + retrieved_descriptions, query_label + retrieved_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = projected_embeddings.filter(pl.col(\"label\") == \"lamb\")[\"index\"][0]\n",
    "\n",
    "display_retrieval_results(projected_embeddings, description_projected_embeddings, image_object_projected_embeddings, query_index, \"image\")\n",
    "display_retrieval_results(projected_embeddings, description_projected_embeddings, image_object_projected_embeddings, query_index, \"description\")\n",
    "\n",
    "display_retrieval_results(clip_embeddings, description_clip_embeddings, image_object_clip_embeddings, query_index, \"image\")\n",
    "display_retrieval_results(clip_embeddings, description_clip_embeddings, image_object_clip_embeddings, query_index, \"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=20):\n",
    "    display(projected_embeddings.sort(\"probability\", descending=True).unique(subset=[\"description\"], keep=\"first\")[\"label\"].value_counts().filter(pl.col(\"count\") == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16910a5",
   "metadata": {},
   "source": [
    "### 5. Analyze the attributes of the retrieved items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_identical_feature_value(\n",
    "    embeddings_data, description_embeddings, image_object_embeddings, feature_name, query_index, query_type, top_k=5\n",
    "):\n",
    "\n",
    "    if query_type == \"description\":\n",
    "        # get object image embeddings based on description embedding\n",
    "        description_query = embeddings_data[\"text\"][query_index]\n",
    "        relevant_document_ids = embeddings_data.filter(pl.col(\"text\") == description_query)[\n",
    "            \"index\"\n",
    "        ].to_numpy()\n",
    "\n",
    "        similarities = cosine_similarity(\n",
    "            np.array(description_embeddings[query_index]).reshape(1, -1),\n",
    "            np.array(image_object_embeddings),\n",
    "        )[0]\n",
    "\n",
    "        ranked_document_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    elif query_type == \"image\":\n",
    "        # get description embedding based on image object embedding\n",
    "        corresponding_descriptions = embeddings_data[\"text\"][query_index]\n",
    "        relevant_document_ids = embeddings_data.filter(\n",
    "            pl.col(\"text\") == corresponding_descriptions\n",
    "        )[\"index\"].to_numpy()\n",
    "\n",
    "        similarities = cosine_similarity(\n",
    "            np.array(image_object_embeddings[query_index]).reshape(1, -1),\n",
    "            np.array(description_embeddings),\n",
    "        )[0]\n",
    "\n",
    "        ranked_document_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    else:\n",
    "        raise NameError(\"This query type does not exist.\")\n",
    "    \n",
    "    feature_query_item = embeddings_data.filter(pl.col(\"index\").is_in(relevant_document_ids))[feature_name][0]\n",
    "    feature_retrieved_items = embeddings_data.filter(pl.col(\"index\").is_in(ranked_document_indices))[feature_name].to_numpy()\n",
    "\n",
    "    percentage_same_feature = (feature_query_item == feature_retrieved_items).sum() / top_k\n",
    "    \n",
    "    return percentage_same_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_identical_feature_value_wrapper(embeddings, feature, modality):\n",
    "    feature_values = (\n",
    "            embeddings.filter(pl.col(feature).is_not_null())[feature]\n",
    "            .value_counts()\n",
    "            .sort(\"count\", descending=True)[:10][feature]\n",
    "            .to_list()\n",
    "        )\n",
    "    \n",
    "    selected_embeddings = embeddings.filter(pl.col(feature).is_in(feature_values)).drop(\"index\").with_row_index()\n",
    "\n",
    "    description_embeddings = selected_embeddings[\"text_embedding_enhanced\"].to_list()\n",
    "    image_object_embeddings = selected_embeddings[\"embedding_object_image\"].to_list()\n",
    "\n",
    "    items_same_feature_value = np.array([])\n",
    "\n",
    "    for i in range(selected_embeddings.shape[0]):\n",
    "        items_same_feature_value = np.append(items_same_feature_value, quantify_identical_feature_value(selected_embeddings, description_embeddings, image_object_embeddings, feature, i, modality))\n",
    "\n",
    "    avg_percentage_same_feature_value = items_same_feature_value.mean()\n",
    "    print(f\"{modality} - {feature}: {avg_percentage_same_feature_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6949379",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Projected Grounding DINO embeddings\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"coarse_type\", \"image\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"first_style\", \"image\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"century\", \"image\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"label\", \"image\")\n",
    "print(\"---\")\n",
    "\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"coarse_type\", \"description\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"first_style\", \"description\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"century\", \"description\")\n",
    "quantify_identical_feature_value_wrapper(projected_embeddings, \"label\", \"description\")\n",
    "\n",
    "print(\"\\nCLIP embeddings\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"coarse_type\", \"image\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"first_style\", \"image\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"century\", \"image\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"label\", \"image\")\n",
    "print(\"---\")\n",
    "\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"coarse_type\", \"description\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"first_style\", \"description\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"century\", \"description\")\n",
    "quantify_identical_feature_value_wrapper(clip_embeddings, \"label\", \"description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f215aaf",
   "metadata": {},
   "source": [
    "### 6. Compare embedding methods statistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_embeddings = (\n",
    "    pl.read_json(\n",
    "        f\"{INPUT_PATH_EMBEDDINGS}baseline_embeddings_test_projections_text_embedding_enhanced_features.json\",\n",
    "        infer_schema_length=1000,\n",
    "    ).rename({\"object_description\": \"text\"}).with_row_index()\n",
    ")\n",
    "description_projected_embeddings = projected_embeddings[\"text_embedding_enhanced\"].to_list()\n",
    "image_object_projected_embeddings = projected_embeddings[\"embedding_object_image\"].to_list()\n",
    "projected_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51315da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embeddings = (\n",
    "    pl.read_json(\n",
    "        f\"{INPUT_PATH_EMBEDDINGS}clip_embeddings_test_clip_full_1e_6_diff_lr_not_frozen_features.json\",\n",
    "        infer_schema_length=1000,\n",
    "    ).rename({\"object_description\": \"text\"}).drop(\"index\").with_row_index()\n",
    ")\n",
    "description_clip_embeddings = clip_embeddings[\"text_embedding_enhanced\"].to_list()\n",
    "image_object_clip_embeddings = clip_embeddings[\"embedding_object_image\"].to_list()\n",
    "clip_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank_biserial_correlation(clip_ranks, grounding_dino_ranks):\n",
    "    clip_ranks = np.array(clip_ranks, dtype=float)\n",
    "    grounding_dino_ranks = np.array(grounding_dino_ranks, dtype=float)\n",
    "\n",
    "    rank_diffs = clip_ranks - grounding_dino_ranks\n",
    "\n",
    "    n_pos = np.sum(rank_diffs > 0)\n",
    "    n_neg = np.sum(rank_diffs < 0)\n",
    "    n_total = len(rank_diffs)\n",
    "\n",
    "    rank_biserial_correlation_value = (n_pos - n_neg) / n_total\n",
    "    print(f\"Rank-biserial correlation: {rank_biserial_correlation_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality = \"image\"\n",
    "clip_ranks = []\n",
    "grounding_dino_ranks = []\n",
    "\n",
    "for query_index in tqdm(range(clip_embeddings.shape[0])):\n",
    "    relevant_document_ids, ranked_document_indices, _ = retrieve_documents(\n",
    "        clip_embeddings, description_clip_embeddings, image_object_clip_embeddings, query_index, modality\n",
    "    )\n",
    "    clip_ranks.append(int(np.where(np.isin(ranked_document_indices, relevant_document_ids) == True)[0][0] + 1))\n",
    "\n",
    "    relevant_document_ids, ranked_document_indices, _ = retrieve_documents(\n",
    "        projected_embeddings, description_projected_embeddings, image_object_projected_embeddings, query_index, modality\n",
    "    )\n",
    "    grounding_dino_ranks.append(int(np.where(np.isin(ranked_document_indices, relevant_document_ids) == True)[0][0] + 1))\n",
    "\n",
    "statistic, p_value = wilcoxon(clip_ranks, grounding_dino_ranks, zero_method=\"wilcox\", alternative=\"two-sided\")\n",
    "print(f\"Statistic: {statistic} - p-value: {p_value}\")\n",
    "compute_rank_biserial_correlation(clip_ranks, grounding_dino_ranks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
