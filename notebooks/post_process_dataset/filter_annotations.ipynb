{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206cfbd2",
   "metadata": {},
   "source": [
    "# Filter annotations\n",
    "Filter short descriptions, filter annotations based on the judge's scores and analyze the annotation quality at the painting level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e466f1f",
   "metadata": {},
   "source": [
    "### 0. Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1905ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import json \n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pprint import pprint\n",
    "import plotly.express as px\n",
    "\n",
    "COLORS = [\"#cd968e\", \"#acb0e0\", \"#aecbdc\", \"#bcd5c3\", \"#bfbfbf\"]\n",
    "ANNOTATIONS_PATH = \"../../data/annotations/\"\n",
    "INTERMEDIATE_DATA_PATH = \"../../data/intermediate/filtered_paintings/\"\n",
    "\n",
    "sys.path.append(\"../annotate_dataset/\")\n",
    "\n",
    "from ground_objects import *\n",
    "from annotate_paintings_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ANNOTATIONS_PATH}unfiltered_annotations.json\") as f:\n",
    "    all_annotations = json.load(f)\n",
    "\n",
    "paintings_data = pl.read_json(f\"{INTERMEDIATE_DATA_PATH}filtered_paintings_enhanced_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60cfc9a",
   "metadata": {},
   "source": [
    "### 1. Filter short descriptions, filter judge outputs and re-compute judge scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b788893",
   "metadata": {},
   "source": [
    "#### 1.1. Filter short descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85286b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_informative_text(obj, text):\n",
    "    informative_words = [\n",
    "        word\n",
    "        for word in list(\n",
    "            set(clean_object_name(text).split(\" \")).difference(\n",
    "                set(clean_object_name(obj).split(\" \"))\n",
    "            )\n",
    "        )\n",
    "        if len(word) != 0\n",
    "    ]\n",
    "\n",
    "    if len(informative_words) <= 1 and len(text) > 0:\n",
    "        # print(obj, \"|\", text, \"|\", informative_words)\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee905ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation_index in range(len(all_annotations)):\n",
    "    kept_description_judgement = {\n",
    "        \"factual_accuracy\": [],\n",
    "        \"coherence\": [],\n",
    "        \"grounding_potential\": [],\n",
    "        \"completeness\": [],\n",
    "    }\n",
    "\n",
    "    for index, (obj, desc) in enumerate(\n",
    "        {\n",
    "            obj: desc\n",
    "            for obj, desc in all_annotations[annotation_index][\"objects\"].items()\n",
    "            if len(desc[1]) != 0\n",
    "        }.items()\n",
    "    ):\n",
    "        if not is_informative_text(obj, desc[-1]):\n",
    "            all_annotations[annotation_index][\"objects\"][obj] = [[\"\"], \"\"]\n",
    "\n",
    "        else:\n",
    "            description_judgement = all_annotations[annotation_index][\"description_judgement\"]\n",
    "            for criterion in kept_description_judgement.keys():\n",
    "                kept_description_judgement[criterion].append(\n",
    "                    description_judgement[criterion][index]\n",
    "                )\n",
    "\n",
    "    all_annotations[annotation_index][\"description_judgement\"][\"factual_accuracy\"] = (\n",
    "        kept_description_judgement[\"factual_accuracy\"]\n",
    "    )\n",
    "    all_annotations[annotation_index][\"description_judgement\"][\"coherence\"] = (\n",
    "        kept_description_judgement[\"coherence\"]\n",
    "    )\n",
    "    all_annotations[annotation_index][\"description_judgement\"][\"grounding_potential\"] = (\n",
    "        kept_description_judgement[\"grounding_potential\"]\n",
    "    )\n",
    "    all_annotations[annotation_index][\"description_judgement\"][\"completeness\"] = (\n",
    "        kept_description_judgement[\"completeness\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1e8ec",
   "metadata": {},
   "source": [
    "#### 1.2. Filter judge outputs and re-compute judge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_fuzzy(object_name, objects):\n",
    "    cleaned_object = clean_object_name(object_name)\n",
    "\n",
    "    for current_object in objects:\n",
    "        cleaned_current_object = clean_object_name(current_object)\n",
    "\n",
    "        if set(cleaned_object.split(\" \")).issubset(set(cleaned_current_object.split(\" \"))) or set(\n",
    "            cleaned_current_object.split(\" \")\n",
    "        ).issubset(set(cleaned_object.split(\" \"))):\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sublist_slice(object_words, description_words):\n",
    "    descriptions_worlds_no = len(description_words)\n",
    "    objects_words_no = len(object_words)\n",
    "\n",
    "    if objects_words_no == 0:\n",
    "        return True\n",
    "\n",
    "    if objects_words_no > descriptions_worlds_no:\n",
    "        return False\n",
    "\n",
    "    for i in range(descriptions_worlds_no - objects_words_no + 1):\n",
    "        if description_words[i : i + objects_words_no] == object_words:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6682e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation_index in range(len(all_annotations)):\n",
    "    annotation = all_annotations[annotation_index]\n",
    "    extraction_judgement = annotation[\"extraction_judgement\"]\n",
    "\n",
    "    description = paintings_data.filter(pl.col(\"id\") == annotation[\"painting_id\"])[\"description\"][0]\n",
    "\n",
    "    tp_objects = list(annotation[\"objects\"].keys())\n",
    "\n",
    "    tp_spans = []\n",
    "\n",
    "    for spans in annotation[\"objects\"].values():\n",
    "\n",
    "        tp_spans.extend([span for span in spans[0] if len(span) != 0])\n",
    "    fn_objects = []\n",
    "\n",
    "    for fn_object in extraction_judgement[\"false_negative_objects\"]:\n",
    "\n",
    "        if not match_fuzzy(fn_object, tp_objects) and is_sublist_slice(\n",
    "            fn_object.split(\" \"), clean_object_name(description).split(\" \")\n",
    "        ):\n",
    "\n",
    "            fn_objects.append(fn_object)\n",
    "    for fn_object, _ in extraction_judgement[\"false_negative_spans\"]:\n",
    "\n",
    "        if (\n",
    "            not match_fuzzy(fn_object, tp_objects)\n",
    "            and is_sublist_slice(fn_object.split(\" \"), clean_object_name(description).split(\" \"))\n",
    "            and fn_object not in fn_objects\n",
    "        ):\n",
    "\n",
    "            fn_objects.append(fn_object)\n",
    "    fn_spans = []\n",
    "\n",
    "    for object_name, spans in extraction_judgement[\"false_negative_spans\"]:\n",
    "\n",
    "        if object_name in fn_objects:\n",
    "\n",
    "            kept_spans = []\n",
    "\n",
    "            for span in spans:\n",
    "\n",
    "                if (\n",
    "                    is_informative_text(object_name, span)\n",
    "                    and span in description\n",
    "                    and len(span) != 0\n",
    "                    and not match_fuzzy(span, tp_spans)\n",
    "                ):\n",
    "\n",
    "                    kept_spans.append(span)\n",
    "            if len(kept_spans) != 0:\n",
    "\n",
    "                fn_spans.append([object_name, kept_spans])\n",
    "    fp_objects = []\n",
    "\n",
    "    for fp_object in extraction_judgement[\"false_positive_objects\"]:\n",
    "\n",
    "        if not match_fuzzy(fp_object, extraction_judgement[\"false_negative_objects\"]) and is_sublist_slice(\n",
    "            fp_object.split(\" \"), clean_object_name(description).split(\" \")\n",
    "        ):\n",
    "\n",
    "            fp_objects.append(fp_object)\n",
    "    for fp_object, _ in extraction_judgement[\"false_positive_spans\"]:\n",
    "\n",
    "        if (\n",
    "            not match_fuzzy(fp_object, extraction_judgement[\"false_negative_objects\"])\n",
    "            and is_sublist_slice(fp_object.split(\" \"), clean_object_name(description).split(\" \"))\n",
    "            and fp_object not in fp_objects\n",
    "        ):\n",
    "\n",
    "            fp_objects.append(fp_object)\n",
    "    fp_spans = []\n",
    "\n",
    "    for object_name, spans in extraction_judgement[\"false_positive_spans\"]:\n",
    "\n",
    "        if object_name in fp_objects:\n",
    "\n",
    "            kept_spans = []\n",
    "\n",
    "            for span in spans:\n",
    "\n",
    "                if (\n",
    "                    is_informative_text(object_name, span)\n",
    "                    and span in description\n",
    "                    and span in tp_spans\n",
    "                    and len(span) != 0\n",
    "                ):\n",
    "\n",
    "                    kept_spans.append(span)\n",
    "            if len(kept_spans) != 0:\n",
    "\n",
    "                fp_spans.append([object_name, kept_spans])\n",
    "    flatten_fp_spans = []\n",
    "\n",
    "    for fp_span in fp_spans:\n",
    "\n",
    "        flatten_fp_spans.extend(fp_span[1])\n",
    "    flatten_fn_spans = []\n",
    "\n",
    "    for fn_span in fn_spans:\n",
    "\n",
    "        flatten_fn_spans.extend(fn_span[1])\n",
    "    tpo_count = len(\n",
    "        list(set(tp_objects).difference(set([clean_object_name(object_name) for object_name in fp_objects])))\n",
    "    )\n",
    "\n",
    "    tps_count = len(list(set(tp_spans).difference(set(flatten_fp_spans))))\n",
    "\n",
    "    fpo_count = len(fp_objects)\n",
    "\n",
    "    fps_count = len(flatten_fp_spans)\n",
    "\n",
    "    fno_count = len(fn_objects)\n",
    "\n",
    "    fns_count = len(flatten_fn_spans)\n",
    "\n",
    "    if tpo_count + fpo_count != 0:\n",
    "\n",
    "        objects_precision = tpo_count / (tpo_count + fpo_count)\n",
    "    else:\n",
    "\n",
    "        objects_precision = 0.0\n",
    "    if fps_count == 0:\n",
    "\n",
    "        spans_precision = 1.0\n",
    "    elif tps_count + fps_count != 0:\n",
    "\n",
    "        spans_precision = tps_count / (tps_count + fps_count)\n",
    "    else:\n",
    "\n",
    "        spans_precision = 0.0\n",
    "    if tpo_count + fno_count != 0:\n",
    "\n",
    "        objects_recall = tpo_count / (tpo_count + fno_count)\n",
    "    else:\n",
    "\n",
    "        objects_recall = 0.0\n",
    "    if fns_count == 0:\n",
    "\n",
    "        spans_recall = 1.0\n",
    "    elif tps_count + fns_count != 0:\n",
    "\n",
    "        spans_recall = tps_count / (tps_count + fns_count)\n",
    "    else:\n",
    "\n",
    "        spans_recall = 0.0\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"false_negative_objects\"] = fn_objects\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"false_negative_spans\"] = fn_spans\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"false_positive_objects\"] = fp_objects\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"false_positive_spans\"] = fp_spans\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"objects_precision\"] = objects_precision\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"objects_recall\"] = objects_recall\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"spans_recall\"] = spans_recall\n",
    "\n",
    "    all_annotations[annotation_index][\"extraction_judgement\"][\"spans_precision\"] = spans_precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c372bd5",
   "metadata": {},
   "source": [
    "### 2. Compute aggregated judge scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ab205",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_scores = {\n",
    "    \"painting_id\": [],\n",
    "    \"objects_recall\": [],\n",
    "    \"objects_precision\": [],\n",
    "    \"objects_f0.5\": [],\n",
    "    \"spans_recall\": [],\n",
    "    \"spans_precision\": [],\n",
    "    \"spans_f0.5\": [],\n",
    "    \"extraction_score\": [],\n",
    "    \"factual_accuracy\": [],\n",
    "    \"coherence\": [],\n",
    "    \"completeness\": [],\n",
    "    \"description_score\": [],\n",
    "    \"annotation_score\": [],\n",
    "}\n",
    "\n",
    "for annotation in all_annotations:\n",
    "    judge_scores[\"painting_id\"].append(annotation[\"painting_id\"])\n",
    "    judge_scores[\"objects_recall\"].append(annotation[\"extraction_judgement\"][\"objects_recall\"])\n",
    "    judge_scores[\"objects_precision\"].append(\n",
    "        annotation[\"extraction_judgement\"][\"objects_precision\"]\n",
    "    )\n",
    "\n",
    "    if judge_scores[\"objects_recall\"][-1] + judge_scores[\"objects_precision\"][-1] == 0:\n",
    "        judge_scores[\"objects_f0.5\"].append(0.0)\n",
    "    else:\n",
    "        judge_scores[\"objects_f0.5\"].append(\n",
    "            (\n",
    "                (1 + 0.5**2)\n",
    "                * judge_scores[\"objects_recall\"][-1]\n",
    "                * judge_scores[\"objects_precision\"][-1]\n",
    "            )\n",
    "            / (judge_scores[\"objects_recall\"][-1] + 0.5**2 * judge_scores[\"objects_precision\"][-1])\n",
    "        )\n",
    "\n",
    "    judge_scores[\"spans_recall\"].append(annotation[\"extraction_judgement\"][\"spans_recall\"])\n",
    "    judge_scores[\"spans_precision\"].append(annotation[\"extraction_judgement\"][\"spans_precision\"])\n",
    "\n",
    "    if judge_scores[\"spans_recall\"][-1] + judge_scores[\"spans_precision\"][-1] == 0:\n",
    "        judge_scores[\"spans_f0.5\"].append(0.0)\n",
    "    else:\n",
    "        judge_scores[\"spans_f0.5\"].append(\n",
    "            ((1 + 0.5**2) * judge_scores[\"spans_recall\"][-1] * judge_scores[\"spans_precision\"][-1])\n",
    "            / (judge_scores[\"spans_recall\"][-1] + 0.5**2 * judge_scores[\"spans_precision\"][-1])\n",
    "        )\n",
    "\n",
    "    judge_scores[\"extraction_score\"].append(\n",
    "        (judge_scores[\"objects_f0.5\"][-1] + judge_scores[\"spans_f0.5\"][-1]) / 2\n",
    "    )\n",
    "\n",
    "    if len(annotation[\"description_judgement\"][\"factual_accuracy\"]) != 0:\n",
    "        judge_scores[\"factual_accuracy\"].append(\n",
    "            np.array(annotation[\"description_judgement\"][\"factual_accuracy\"]).mean()\n",
    "        )\n",
    "    else:\n",
    "        judge_scores[\"factual_accuracy\"].append(5.0)\n",
    "\n",
    "    if len(annotation[\"description_judgement\"][\"coherence\"]) != 0:\n",
    "        judge_scores[\"coherence\"].append(\n",
    "            np.array(annotation[\"description_judgement\"][\"coherence\"]).mean()\n",
    "        )\n",
    "    else:\n",
    "        judge_scores[\"coherence\"].append(5.0)\n",
    "\n",
    "    if len(annotation[\"description_judgement\"][\"completeness\"]) != 0:\n",
    "        judge_scores[\"completeness\"].append(\n",
    "            np.array(annotation[\"description_judgement\"][\"factual_accuracy\"]).mean()\n",
    "        )\n",
    "    else:\n",
    "        judge_scores[\"completeness\"].append(5.0)\n",
    "\n",
    "    judge_scores[\"description_score\"].append(\n",
    "        (\n",
    "            judge_scores[\"factual_accuracy\"][-1]\n",
    "            + judge_scores[\"coherence\"][-1]\n",
    "            + judge_scores[\"completeness\"][-1]\n",
    "        )\n",
    "        / 3\n",
    "    )\n",
    "\n",
    "    judge_scores[\"annotation_score\"].append(\n",
    "        (\n",
    "            (\n",
    "                judge_scores[\"objects_f0.5\"][-1]\n",
    "                + judge_scores[\"spans_f0.5\"][-1]\n",
    "                + ((judge_scores[\"description_score\"][-1] - 1) / 4)\n",
    "            )\n",
    "            / 3\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "judge_scores_df = pl.from_dict(judge_scores)\n",
    "judge_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681dfc0a",
   "metadata": {},
   "source": [
    "### 3. Analyze aggregated judge scores and filter the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iqr_bounds(scores):\n",
    "    data_array = scores\n",
    "\n",
    "    Q1 = np.percentile(data_array, 25)\n",
    "    Q3 = np.percentile(data_array, 75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_description(description, words_per_line=25):\n",
    "    description_words = description.replace(\"\\n\", \" \").split(\" \")\n",
    "\n",
    "    for i in range(0, len(description_words), words_per_line):\n",
    "        line = \" \".join(description_words[i : i + words_per_line])\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distribution(judge_scores, score_name):\n",
    "    fig = px.histogram(\n",
    "        [val for val in judge_scores[score_name] if val != -1],\n",
    "        marginal=\"box\",\n",
    "        title=f\"Distribution of {score_name} scores\",\n",
    "        labels={\"value\": f\"{score_name} value\"},\n",
    "        nbins=100,\n",
    "        color_discrete_sequence=COLORS[1:2],\n",
    "    )\n",
    "\n",
    "    fig.update_traces(showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3774bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_correlation_matrix(judge_scores_df):\n",
    "    fig = px.imshow(\n",
    "\n",
    "        judge_scores_df.select(\n",
    "            \"objects_f0.5\",\n",
    "            \"spans_f0.5\",\n",
    "            \"factual_accuracy\",\n",
    "            \"coherence\",\n",
    "            \"completeness\",\n",
    "            \"description_score\",\n",
    "            \"annotation_score\",\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .corr(),\n",
    "\n",
    "        text_auto=True,\n",
    "        color_continuous_scale=[COLORS[0], COLORS[2]],\n",
    "        range_color=[-1, 1],\n",
    "        title=\"Correlation Matrix of Judge Evaluation Scores\",\n",
    "\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=10, r=10, t=40, b=10),\n",
    "        height=500 * 1.5,\n",
    "        width=625 * 1.5,\n",
    "        title_x=0.47,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781af777",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_correlation_matrix(judge_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_distribution(judge_scores, \"spans_f0.5\")\n",
    "plot_score_distribution(judge_scores, \"objects_f0.5\")\n",
    "plot_score_distribution(judge_scores, \"extraction_score\")\n",
    "plot_score_distribution(judge_scores, \"factual_accuracy\")\n",
    "plot_score_distribution(judge_scores, \"coherence\")\n",
    "plot_score_distribution(judge_scores, \"completeness\")\n",
    "plot_score_distribution(judge_scores, \"description_score\")\n",
    "plot_score_distribution(judge_scores, \"annotation_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af18d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = get_iqr_bounds(judge_scores[\"description_score\"])\n",
    "painting_ids = (\n",
    "    judge_scores_df.filter((pl.col(\"description_score\") < 3.5))\n",
    "    # judge_scores_df.filter((pl.col(\"description_score\") >= 3) & (pl.col(\"description_score\") < 3.5))\n",
    "    .sample(fraction=1.0, shuffle=True, seed=42)[\"painting_id\"].to_list()[:3]\n",
    ")\n",
    "painting_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "for painting_id in painting_ids:\n",
    "    _, image = load_image(painting_id)\n",
    "\n",
    "    annotation = [\n",
    "        annotation for annotation in all_annotations if annotation[\"painting_id\"] == painting_id\n",
    "    ][0]\n",
    "    description = paintings_data.filter(pl.col(\"id\") == painting_id)[\"description\"][0]\n",
    "\n",
    "    print(annotation[\"painting_id\"])\n",
    "    print_description(description)\n",
    "    pprint(annotation[\"objects\"], indent=2)\n",
    "    pprint(annotation[\"description_judgement\"], indent=2)\n",
    "    pprint(annotation[\"extraction_judgement\"], indent=2)\n",
    "    display_annotated_image(image, annotation[\"bounding_boxes\"], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da547831",
   "metadata": {},
   "source": [
    "#### 3.1. Filter paintings with low-quality objects and spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49063bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_scores_df = judge_scores_df.filter(\n",
    "    ~(\n",
    "        ((pl.col(\"objects_f0.5\") == 0) & (pl.col(\"spans_f0.5\") == 0))\n",
    "        | (pl.col(\"extraction_score\") <= 0.215)\n",
    "    )\n",
    ")\n",
    "judge_scores = judge_scores_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_correlation_matrix(judge_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2414227",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_distribution(judge_scores, \"spans_f0.5\")\n",
    "plot_score_distribution(judge_scores, \"objects_f0.5\")\n",
    "plot_score_distribution(judge_scores, \"extraction_score\")\n",
    "plot_score_distribution(judge_scores, \"factual_accuracy\")\n",
    "plot_score_distribution(judge_scores, \"coherence\")\n",
    "plot_score_distribution(judge_scores, \"completeness\")\n",
    "plot_score_distribution(judge_scores, \"description_score\")\n",
    "plot_score_distribution(judge_scores, \"annotation_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = []\n",
    "completeness = []\n",
    "factual_accuracy = []\n",
    "kept_painting_ids = judge_scores[\"painting_id\"].to_list()\n",
    "\n",
    "for annotation_index in range(len(all_annotations)):\n",
    "    if all_annotations[annotation_index][\"painting_id\"] not in kept_painting_ids:\n",
    "        continue\n",
    "\n",
    "    coherence.extend(all_annotations[annotation_index][\"description_judgement\"][\"coherence\"])\n",
    "    completeness.extend(all_annotations[annotation_index][\"description_judgement\"][\"completeness\"])\n",
    "    factual_accuracy.extend(all_annotations[annotation_index][\"description_judgement\"][\"factual_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5188d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    coherence,\n",
    "    marginal=\"box\",\n",
    "    nbins=100,\n",
    "    color_discrete_sequence=COLORS[1:2],\n",
    ")\n",
    "\n",
    "fig.update_traces(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fece5d",
   "metadata": {},
   "source": [
    "#### 3.2. Filter descriptions with low quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_annotations = []\n",
    "removed_descriptions_counter = 0\n",
    "affected_paintings_counter = 0\n",
    "kept_painting_ids = judge_scores[\"painting_id\"].to_list()\n",
    "\n",
    "for annotation_index in range(len(all_annotations)):\n",
    "    if all_annotations[annotation_index][\"painting_id\"] not in kept_painting_ids:\n",
    "        continue\n",
    "\n",
    "    updated_annotation = {}\n",
    "    updated_annotation[\"painting_id\"] = all_annotations[annotation_index][\"painting_id\"]\n",
    "    updated_annotation[\"bounding_boxes\"] = all_annotations[annotation_index][\"bounding_boxes\"]\n",
    "\n",
    "    updated_objects = {}\n",
    "    description_judgement_index = 0\n",
    "\n",
    "    removed_description = False\n",
    "    for obj, desc in all_annotations[annotation_index][\"objects\"].items():\n",
    "        if len(desc[-1]) == 0:\n",
    "            updated_objects[obj] = desc\n",
    "        else:\n",
    "            if (\n",
    "                all_annotations[annotation_index][\"description_judgement\"][\"factual_accuracy\"][\n",
    "                    description_judgement_index\n",
    "                ]\n",
    "                >= 3\n",
    "                and all_annotations[annotation_index][\"description_judgement\"][\"coherence\"][\n",
    "                    description_judgement_index\n",
    "                ]\n",
    "                >= 3\n",
    "                and all_annotations[annotation_index][\"description_judgement\"][\"completeness\"][\n",
    "                    description_judgement_index\n",
    "                ]\n",
    "                >= 3\n",
    "            ):\n",
    "                updated_objects[obj] = desc\n",
    "            else:\n",
    "                updated_objects[obj] = [[\"\"], \"\"]\n",
    "                removed_descriptions_counter += 1\n",
    "                removed_description = True\n",
    "\n",
    "            description_judgement_index += 1\n",
    "\n",
    "    if removed_description:\n",
    "        affected_paintings_counter += 1\n",
    "\n",
    "    updated_annotation[\"objects\"] = updated_objects\n",
    "    filtered_annotations.append(updated_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6031df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of descriptions removed: {removed_descriptions_counter}\")\n",
    "print(f\"Number of affected paintings: {affected_paintings_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae494be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{ANNOTATIONS_PATH}filtered_annotations.json\", \"w\") as f:\n",
    "    json.dump(filtered_annotations, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhance_vg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
